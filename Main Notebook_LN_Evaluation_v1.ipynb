{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f096d7-10e2-46cf-8585-f26f401ba090",
   "metadata": {
    "id": "74f096d7-10e2-46cf-8585-f26f401ba090"
   },
   "source": [
    "# 1. Datenaufbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bddd8232-3bb0-4279-aa58-8c99b3d1b977",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 695
    },
    "id": "bddd8232-3bb0-4279-aa58-8c99b3d1b977",
    "outputId": "1b2f930e-9510-4ef4-8f74-95b074cca101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-4.3.1-cp39-cp39-win_amd64.whl (24.0 MB)\n",
      "Collecting numpy>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.24.3-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Collecting scipy>=1.7.0 (from gensim)\n",
      "  Using cached scipy-1.10.1-cp39-cp39-win_amd64.whl (42.5 MB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Installing collected packages: smart-open, numpy, scipy, gensim\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 6.3.0\n",
      "    Uninstalling smart-open-6.3.0:\n",
      "      Successfully uninstalled smart-open-6.3.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.1\n",
      "    Uninstalling scipy-1.10.1:\n",
      "      Successfully uninstalled scipy-1.10.1\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.3.1Note: you may need to restart the kernel to use updated packages.\n",
      "    Uninstalling gensim-4.3.1:\n",
      "\n",
      "      Successfully uninstalled gensim-4.3.1\n",
      "Successfully installed gensim-4.3.1 numpy-1.24.3 scipy-1.10.1 smart-open-6.3.0\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --force-reinstall gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40098027-2663-43a4-965d-93fba60db100",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40098027-2663-43a4-965d-93fba60db100",
    "outputId": "beb878f0-3f10-4631-82d4-bb003dc57a18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-1.24.3-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "Successfully installed numpy-1.24.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --force-reinstall numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vVLXXj1lxA-B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "vVLXXj1lxA-B",
    "outputId": "6bc4e0c5-02cd-49bd-e81f-91b2a96914a8"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files \n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "HgjNb_t9xbLs",
   "metadata": {
    "id": "HgjNb_t9xbLs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f46d7a0-4fe5-40ed-9148-f11620646145",
   "metadata": {
    "id": "2f46d7a0-4fe5-40ed-9148-f11620646145"
   },
   "outputs": [],
   "source": [
    "def helper(column):\n",
    "    return str(column).split(\",\")\n",
    "\n",
    "def open_csv_default():\n",
    "    df_data = pd.read_csv(\"naukri_data_science_jobs_india.csv\")\n",
    "    df_data = df_data.drop(columns=['Company', 'Location', 'Job Experience'])\n",
    "    df_data[\"Skills/Description\"] = df_data[\"Skills/Description\"].apply(lambda x: helper(x))\n",
    "    return df_data\n",
    "\n",
    "def open_csv_clean():\n",
    "    df_data = pd.read_csv(\"naukri_data_science_jobs_india_cleaned.csv\")\n",
    "    formatted = df_data.values.tolist()\n",
    "    return formatted\n",
    "\n",
    "def open_csv_clustered():\n",
    "    df_data = pd.read_csv(\"naukri_data_science_jobs_india_cleaned_clusterd.csv\")\n",
    "    df_data = df_data.drop(columns=['Job_Role', 'lda_score'])\n",
    "    df_data = df_data[['lda_topic', 'Skills/Description']]\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e8718-046f-4085-9203-91eaa329054d",
   "metadata": {},
   "source": [
    "# 2. Methodendeklaration, Hilfsfunktionen unsw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4z3acAgC4Qf",
   "metadata": {
    "id": "d4z3acAgC4Qf"
   },
   "outputs": [],
   "source": [
    "### Training uses the Doc2Vec or Word2Vec model which simply embeds words according to how often they occur together (ignoring order afaik)\n",
    "## Adapt vector size and epochs in training once data set is complete\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import ast\n",
    "import numpy\n",
    "\n",
    "def trainWord2Vec(data):\n",
    "# Preprocess the data and create list of skills (this training does NOT include jobs!)\n",
    "    tagged_data = [ast.literal_eval(skills) for job, skills in data]\n",
    "\n",
    "# Train the Word2Vec model, try different vector sizes for interesting effects in similarities\n",
    "    model = Word2Vec(vector_size=50, min_count=1, workers=4, epochs=20)\n",
    "    model.build_vocab(tagged_data)\n",
    "\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def trainDoc2Vec(data):\n",
    "    tagged_data = [TaggedDocument(words=ast.literal_eval(skills), tags=[job]) for job, skills in data[:]]\n",
    "\n",
    "# Train the Doc2Vec model\n",
    "    model = Doc2Vec(vector_size=100, window=5, min_count=1, workers=4, epochs=20)\n",
    "    model.build_vocab(tagged_data)\n",
    "\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "    return model\n",
    "\n",
    "# helper method to calculate average embedding vector of a list of string skills, filters out skills unknown to the model\n",
    "def averageOfSkills(model, input, axis = 0):\n",
    "    vectors = [model.wv[i] for i in input if model.wv.has_index_for(i)]\n",
    "    if(len(vectors) == 0):\n",
    "        return numpy.zeros(len(model.wv[0]))\n",
    "    \n",
    "    return numpy.average(vectors, axis = axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eRXCk52dDg83",
   "metadata": {
    "id": "eRXCk52dDg83"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def calcAvgEmbeddings(model, data):\n",
    "\n",
    "    # Tests for the embedding go here\n",
    "    print(model.wv.most_similar(\"neural_networks\", topn = 5))\n",
    "\n",
    "    #cosine distance\n",
    "    print(model.wv.distance(\"finance\", \"machine_learning\"))\n",
    "    print(model.wv.distance(\"ai\", \"machine_learning\"))\n",
    "    print(model.wv.distance(\"business_intelligence\", \"management\"))\n",
    "    print(model.wv.distance('corporate_governance', 'ai'))\n",
    "    #euclidean distance\n",
    "    #print(numpy.linalg.norm(model.wv[\"statistics\"] - model.wv[\"machine_learning\"]))\n",
    "\n",
    "    # split the tuples\n",
    "    job_titles, skills = list(zip(*data))\n",
    "    # calculate the average skill vector of every job offering / person and zip back together\n",
    "    skillAverages = [averageOfSkills(model, ast.literal_eval(skill)) for skill in skills]\n",
    "    data_averaged = list(zip(job_titles, skillAverages))\n",
    "\n",
    "    return (skillAverages, data_averaged)\n",
    "\n",
    "# This gives us a list of job offerings and average embeddings. \n",
    "# Could be used as input to a graph neural network or knowledge graph?\n",
    "# Is used below to classify immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6MOCIrZkKbDX",
   "metadata": {
    "id": "6MOCIrZkKbDX"
   },
   "outputs": [],
   "source": [
    "# simply suggest smallest distance to user's average (\"Option 2\") using word2Vec\n",
    "import itertools\n",
    "import operator\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def prepareOption2(data):\n",
    "\n",
    "    model = trainWord2Vec(data)\n",
    "    skillAverages, data_averaged = calcAvgEmbeddings(model, data)\n",
    "\n",
    "    # sort before grouping\n",
    "    data_averaged.sort(key=operator.itemgetter(0))\n",
    "    # Group by job title, take the average embedding of everyone with that title and make a dictionary (maps job title to average embedding)\n",
    "    job_averages = {key : numpy.average(list(zip(*list(value)))[1], axis = 0)\n",
    "        for key, value in itertools.groupby(data_averaged, lambda x: x[0])}\n",
    "\n",
    "    #keep keys and values\n",
    "    keys = list(job_averages.keys())\n",
    "    values = list(job_averages.values())\n",
    "    # easy access to the avg vector\n",
    "    #print(job_averages['Advisor, Data Science'])\n",
    "    \n",
    "    # model.save(\"word2vec_model_option2.bin\")\n",
    "\n",
    "    return job_averages, keys, values, model\n",
    "\n",
    "# usage in next cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eYYHhscDE1vV",
   "metadata": {
    "id": "eYYHhscDE1vV"
   },
   "outputs": [],
   "source": [
    "# \"Option 3\" could be using the embedding learned by Doc2Vec and doing the same manual averaging as Option 2 for \"learning\" the correlation of job to skills\n",
    "import itertools\n",
    "import operator\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "\n",
    "def prepareOption3(data):\n",
    "#if !data:\n",
    "#  data = open_csv_clean()\n",
    "    model = trainDoc2Vec(data)\n",
    "    skillAverages, data_averaged = calcAvgEmbeddings(model, data)\n",
    "\n",
    "    # sort before grouping\n",
    "    data_averaged.sort(key=operator.itemgetter(0))\n",
    "    # Group by job title, take the average embedding of everyone with that title and make a dictionary (maps job title to average embedding)\n",
    "    job_averages = {key : numpy.average(list(zip(*list(value)))[1], axis = 0)\n",
    "        for key, value in itertools.groupby(data_averaged, lambda x: x[0])}\n",
    "\n",
    "    #keep keys and values\n",
    "    keys = list(job_averages.keys())\n",
    "    values = list(job_averages.values())\n",
    "    \n",
    "    return job_averages, keys, values, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ddfa1f-44a3-48f3-b710-455134a4b304",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WW1p_fgqFIws",
    "outputId": "7b273ef3-9ecc-4400-d7bb-5ce1eb1a33da"
   },
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ffC6pZCMFed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ffC6pZCMFed",
    "outputId": "9429fbfd-cfa3-43a1-f046-255e2e21d050"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Proper testing\\nimport heapq\\n\\n# divide data into 75 train, 25 test\\ndef train_test_split(df, frac=0.25):\\n    \\n    # get random sample \\n    test = df.sample(frac=frac, axis=0)\\n\\n    # get everything but the test sample\\n    train = df.drop(index=test.index)\\n\\n    return train, test\\n\\ndf = open_csv_clustered()\\n\\ndf_data, df_test = train_test_split(df)\\n\\nformatted_data = df_data.values.tolist()\\nformatted_test = df_test.values.tolist()\\njobs, skills = list(zip(*formatted_test))\\nskills = [ast.literal_eval(skill) for skill in skills]\\n\\nprint(len(skills))\\n\\noption = 1\\n#Doc2Vec\\nif option == 1:\\n    model = trainDoc2Vec(formatted_data)\\n    correct = 0\\n    top3 = 0\\n    for i in range(len(jobs)):\\n        infer_vector = model.infer_vector(skills[i])\\n        similar_jobs = model.dv.most_similar([infer_vector], topn=3)\\n        if(similar_jobs[0][0] == jobs[i]):\\n            correct = correct + 1\\n            top3 = top3 + 1\\n        elif(any(item[0] == jobs[i] for item in similar_jobs)):\\n            top3 = top3 + 1\\n        resultCorrect = correct / len(jobs)\\n        resultTop3 = top3 / len(jobs)\\n        print(\"Option 1: Percent of correct guesses: \" + str(resultCorrect) + \", percent of top 3 guesses: \" + str(resultTop3))\\n\\n#Word2Vec\\nelif option == 2:\\n    job_averages, keys, values, model = prepareOption2(formatted_data)\\n    correct = 0\\n    top3 = 0\\n    for i in range(len(jobs)):\\n        avg = averageOfSkills(model, skills[i])\\n        distVec = cdist([avg,], values)\\n        #sortedList = sorted(list(zip(distVec.transpose(), keys)))\\n        #topJobs = [i for scor,i in sortedList[0:5]]\\n        topJobs = [k for dist, k in heapq.nsmallest(3, zip(distVec.transpose(), keys))]\\n        if(topJobs[0] == jobs[i]):\\n            correct = correct + 1\\n            top3 = top3 + 1\\n        elif(jobs[i] in topJobs):\\n            top3 = top3 + 1\\n    resultCorrect = correct / len(jobs)\\n    resultTop3 = top3 / len(jobs)\\n    print(\"Option 2: Rate of correct guesses: \" + str(resultCorrect) + \", rate of top 3 guesses: \" + str(resultTop3))\\nelif option == 3:\\n    job_averages, keys, values, model = prepareOption3(formatted_data)\\n    correct = 0\\n    top3 = 0\\n    for i in range(len(jobs)):\\n        avg = averageOfSkills(model, skills[i])\\n        distVec = cdist([avg,], values)\\n        #sortedList = sorted(list(zip(distVec.transpose(), keys)))\\n        #topJobs = [i for scor,i in sortedList[0:5]]\\n        topJobs = [k for dist, k in heapq.nsmallest(3, zip(distVec.transpose(), keys))]\\n        if(topJobs[0] == jobs[i]):\\n            correct = correct + 1\\n            top3 = top3 + 1\\n        elif(jobs[i] in topJobs):\\n            top3 = top3 + 1\\n    resultCorrect = correct / len(jobs)\\n    resultTop3 = top3 / len(jobs)\\n    print(\"Option 3: Rate of correct guesses: \" + str(resultCorrect) + \", rate of top 3 guesses: \" + str(resultTop3))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Proper testing\n",
    "import heapq\n",
    "\n",
    "# divide data into 75 train, 25 test\n",
    "def train_test_split(df, frac=0.25):\n",
    "    \n",
    "    # get random sample \n",
    "    test = df.sample(frac=frac, axis=0)\n",
    "\n",
    "    # get everything but the test sample\n",
    "    train = df.drop(index=test.index)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "df = open_csv_clustered()\n",
    "\n",
    "df_data, df_test = train_test_split(df)\n",
    "\n",
    "formatted_data = df_data.values.tolist()\n",
    "formatted_test = df_test.values.tolist()\n",
    "jobs, skills = list(zip(*formatted_test))\n",
    "skills = [ast.literal_eval(skill) for skill in skills]\n",
    "\n",
    "print(len(skills))\n",
    "\n",
    "option = 1\n",
    "#Doc2Vec\n",
    "if option == 1:\n",
    "    model = trainDoc2Vec(formatted_data)\n",
    "    correct = 0\n",
    "    top3 = 0\n",
    "    for i in range(len(jobs)):\n",
    "        infer_vector = model.infer_vector(skills[i])\n",
    "        similar_jobs = model.dv.most_similar([infer_vector], topn=3)\n",
    "        if(similar_jobs[0][0] == jobs[i]):\n",
    "            correct = correct + 1\n",
    "            top3 = top3 + 1\n",
    "        elif(any(item[0] == jobs[i] for item in similar_jobs)):\n",
    "            top3 = top3 + 1\n",
    "        resultCorrect = correct / len(jobs)\n",
    "        resultTop3 = top3 / len(jobs)\n",
    "        print(\"Option 1: Percent of correct guesses: \" + str(resultCorrect) + \", percent of top 3 guesses: \" + str(resultTop3))\n",
    "\n",
    "#Word2Vec\n",
    "elif option == 2:\n",
    "    job_averages, keys, values, model = prepareOption2(formatted_data)\n",
    "    correct = 0\n",
    "    top3 = 0\n",
    "    for i in range(len(jobs)):\n",
    "        avg = averageOfSkills(model, skills[i])\n",
    "        distVec = cdist([avg,], values)\n",
    "        #sortedList = sorted(list(zip(distVec.transpose(), keys)))\n",
    "        #topJobs = [i for scor,i in sortedList[0:5]]\n",
    "        topJobs = [k for dist, k in heapq.nsmallest(3, zip(distVec.transpose(), keys))]\n",
    "        if(topJobs[0] == jobs[i]):\n",
    "            correct = correct + 1\n",
    "            top3 = top3 + 1\n",
    "        elif(jobs[i] in topJobs):\n",
    "            top3 = top3 + 1\n",
    "    resultCorrect = correct / len(jobs)\n",
    "    resultTop3 = top3 / len(jobs)\n",
    "    print(\"Option 2: Rate of correct guesses: \" + str(resultCorrect) + \", rate of top 3 guesses: \" + str(resultTop3))\n",
    "elif option == 3:\n",
    "    job_averages, keys, values, model = prepareOption3(formatted_data)\n",
    "    correct = 0\n",
    "    top3 = 0\n",
    "    for i in range(len(jobs)):\n",
    "        avg = averageOfSkills(model, skills[i])\n",
    "        distVec = cdist([avg,], values)\n",
    "        #sortedList = sorted(list(zip(distVec.transpose(), keys)))\n",
    "        #topJobs = [i for scor,i in sortedList[0:5]]\n",
    "        topJobs = [k for dist, k in heapq.nsmallest(3, zip(distVec.transpose(), keys))]\n",
    "        if(topJobs[0] == jobs[i]):\n",
    "            correct = correct + 1\n",
    "            top3 = top3 + 1\n",
    "        elif(jobs[i] in topJobs):\n",
    "            top3 = top3 + 1\n",
    "    resultCorrect = correct / len(jobs)\n",
    "    resultTop3 = top3 / len(jobs)\n",
    "    print(\"Option 3: Rate of correct guesses: \" + str(resultCorrect) + \", rate of top 3 guesses: \" + str(resultTop3))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1f72efd-1db6-4b70-afbd-e43d3290f334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport heapq\\n\\n# divide data into 75 train, 25 test\\ndef train_test_split(df, frac=0.25):\\n    # get random sample \\n    test = df.sample(frac=frac, axis=0)\\n    # get everything but the test sample\\n    train = df.drop(index=test.index)\\n\\n    return train, test\\n\\ndf = open_csv_clustered()\\n\\ndf_data, df_test = train_test_split(df)\\n\\nformatted_data = df_data.values.tolist()\\nformatted_test = df_test.values.tolist()\\njobs, skills = list(zip(*formatted_test))\\nskills = [ast.literal_eval(skill) for skill in skills]\\n\\n# Option 1: Doc2Vec\\nmodel1 = trainDoc2Vec(formatted_data)\\ntop_jobs_option1 = []\\n\\nfor i in range(len(jobs)):\\n    infer_vector = model1.infer_vector(skills[i])\\n    similar_jobs = model1.dv.most_similar([infer_vector], topn=3)\\n    top_jobs_option1.append(similar_jobs)\\n\\n# Option 2: Word2Vec\\njob_averages, keys, values, model2 = prepareOption2(formatted_data)\\ntop_jobs_option2 = []\\n\\nfor i in range(len(jobs)):\\n    avg = averageOfSkills(model2, skills[i])\\n    distVec = cdist([avg,], values)\\n    top_jobs = [k for dist, k in heapq.nsmallest(3, zip(distVec.transpose(), keys))]\\n    top_jobs_option2.append(top_jobs)\\n\\n# Option 3: Doc2Vec Embedding\\njob_averages, keys, values, model3 = prepareOption3(formatted_data)\\ntop_jobs_option3 = []\\n\\nfor j in range(len(jobs)):\\n    avg = averageOfSkills(model3, skills[j])\\n    distVec = cdist([avg,], values)\\n    top_jobs = [m for dist, m in heapq.nsmallest(3, zip(distVec.transpose(), keys))]\\n    top_jobs_option3.append(top_jobs)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import heapq\n",
    "\n",
    "# divide data into 75 train, 25 test\n",
    "def train_test_split(df, frac=0.25):\n",
    "    # get random sample \n",
    "    test = df.sample(frac=frac, axis=0)\n",
    "    # get everything but the test sample\n",
    "    train = df.drop(index=test.index)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "df = open_csv_clustered()\n",
    "\n",
    "df_data, df_test = train_test_split(df)\n",
    "\n",
    "formatted_data = df_data.values.tolist()\n",
    "formatted_test = df_test.values.tolist()\n",
    "jobs, skills = list(zip(*formatted_test))\n",
    "skills = [ast.literal_eval(skill) for skill in skills]\n",
    "\n",
    "# Option 1: Doc2Vec\n",
    "model1 = trainDoc2Vec(formatted_data)\n",
    "top_jobs_option1 = []\n",
    "\n",
    "for i in range(len(jobs)):\n",
    "    infer_vector = model1.infer_vector(skills[i])\n",
    "    similar_jobs = model1.dv.most_similar([infer_vector], topn=3)\n",
    "    top_jobs_option1.append(similar_jobs)\n",
    "\n",
    "# Option 2: Word2Vec\n",
    "job_averages, keys, values, model2 = prepareOption2(formatted_data)\n",
    "top_jobs_option2 = []\n",
    "\n",
    "for i in range(len(jobs)):\n",
    "    avg = averageOfSkills(model2, skills[i])\n",
    "    distVec = cdist([avg,], values)\n",
    "    top_jobs = [k for dist, k in heapq.nsmallest(3, zip(distVec.transpose(), keys))]\n",
    "    top_jobs_option2.append(top_jobs)\n",
    "\n",
    "# Option 3: Doc2Vec Embedding\n",
    "job_averages, keys, values, model3 = prepareOption3(formatted_data)\n",
    "top_jobs_option3 = []\n",
    "\n",
    "for j in range(len(jobs)):\n",
    "    avg = averageOfSkills(model3, skills[j])\n",
    "    distVec = cdist([avg,], values)\n",
    "    top_jobs = [m for dist, m in heapq.nsmallest(3, zip(distVec.transpose(), keys))]\n",
    "    top_jobs_option3.append(top_jobs)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c5f0526-7619-44a2-b93b-cc050f1d1cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Proper testing\\nimport heapq\\n\\n# divide data into 75 train, 25 test\\ndef train_test_split(df, frac=0.25):\\n    # get random sample \\n    test = df.sample(frac=frac, axis=0)\\n    # get everything but the test sample\\n    train = df.drop(index=test.index)\\n    return train, test\\n\\ndf = open_csv_clustered()\\ndf_data, df_test = train_test_split(df)\\n\\nformatted_data = df_data.values.tolist()\\nformatted_test = df_test.values.tolist()\\njobs, skills = list(zip(*formatted_test))\\nskills = [ast.literal_eval(skill) for skill in skills]\\n\\n\\noption = 3\\n\\n#Doc2Vec\\nif option == 1:\\n    model = trainDoc2Vec(formatted_data)\\n    correct = 0\\n    top3 = 0\\n    for i in range(len(jobs)):\\n        infer_vector = model.infer_vector(skills[i])\\n        similar_jobs = model.dv.most_similar([infer_vector], topn=3)\\n\\n#Word2Vec\\nelif option == 2:\\n    job_averages, keys, values, model = prepareOption2(formatted_data)\\n    for i in range(len(jobs)):\\n        avg = averageOfSkills(model, skills[i])\\n        distVec = cdist([avg,], values)\\n        #sortedList = sorted(list(zip(distVec.transpose(), keys)))\\n        #topJobs = [i for scor,i in sortedList[0:5]]\\n        topJobs = [k for dist, k in heapq.nsmallest(3, zip(distVec.transpose(), keys))]\\nelif option == 3:\\n    job_averages, keys, values, model = prepareOption3(formatted_data)\\n    for i in range(len(jobs)):\\n        avg = averageOfSkills(model, skills[i])\\n        distVec = cdist([avg,], values)\\n        #sortedList = sorted(list(zip(distVec.transpose(), keys)))\\n        #topJobs = [i for scor,i in sortedList[0:5]]\\n        topJobs = [k for dist, k in heapq.nsmallest(3, zip(distVec.transpose(), keys))]\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Proper testing\n",
    "import heapq\n",
    "\n",
    "# divide data into 75 train, 25 test\n",
    "def train_test_split(df, frac=0.25):\n",
    "    # get random sample \n",
    "    test = df.sample(frac=frac, axis=0)\n",
    "    # get everything but the test sample\n",
    "    train = df.drop(index=test.index)\n",
    "    return train, test\n",
    "\n",
    "df = open_csv_clustered()\n",
    "df_data, df_test = train_test_split(df)\n",
    "\n",
    "formatted_data = df_data.values.tolist()\n",
    "formatted_test = df_test.values.tolist()\n",
    "jobs, skills = list(zip(*formatted_test))\n",
    "skills = [ast.literal_eval(skill) for skill in skills]\n",
    "\n",
    "\n",
    "option = 3\n",
    "\n",
    "#Doc2Vec\n",
    "if option == 1:\n",
    "    model = trainDoc2Vec(formatted_data)\n",
    "    correct = 0\n",
    "    top3 = 0\n",
    "    for i in range(len(jobs)):\n",
    "        infer_vector = model.infer_vector(skills[i])\n",
    "        similar_jobs = model.dv.most_similar([infer_vector], topn=3)\n",
    "\n",
    "#Word2Vec\n",
    "elif option == 2:\n",
    "    job_averages, keys, values, model = prepareOption2(formatted_data)\n",
    "    for i in range(len(jobs)):\n",
    "        avg = averageOfSkills(model, skills[i])\n",
    "        distVec = cdist([avg,], values)\n",
    "        #sortedList = sorted(list(zip(distVec.transpose(), keys)))\n",
    "        #topJobs = [i for scor,i in sortedList[0:5]]\n",
    "        topJobs = [k for dist, k in heapq.nsmallest(3, zip(distVec.transpose(), keys))]\n",
    "elif option == 3:\n",
    "    job_averages, keys, values, model = prepareOption3(formatted_data)\n",
    "    for i in range(len(jobs)):\n",
    "        avg = averageOfSkills(model, skills[i])\n",
    "        distVec = cdist([avg,], values)\n",
    "        #sortedList = sorted(list(zip(distVec.transpose(), keys)))\n",
    "        #topJobs = [i for scor,i in sortedList[0:5]]\n",
    "        topJobs = [k for dist, k in heapq.nsmallest(3, zip(distVec.transpose(), keys))]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a5a5060-76e8-4cc2-8513-49dc2d734bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('text_mining', 0.9816309213638306), ('deep_learning', 0.9666792154312134), ('computer_vision', 0.9623832702636719), ('logistic_regression', 0.9456374645233154), ('image_segmentation', 0.9412624835968018)]\n",
      "0.7450214624404907\n",
      "0.34322458505630493\n",
      "0.42939293384552\n",
      "0.48050814867019653\n",
      "[('computer_vision', 0.9698963165283203), ('product_management', 0.9454489350318909), ('statistical_programming', 0.943572461605072), ('predictive_analysis', 0.9377436637878418), ('text_mining', 0.9317780137062073)]\n",
      "0.5712321996688843\n",
      "0.3256204128265381\n",
      "0.5625500082969666\n",
      "0.5768380761146545\n"
     ]
    }
   ],
   "source": [
    "# Proper testing\n",
    "import heapq\n",
    "\n",
    "# divide data into 75 train, 25 test\n",
    "def train_test_split(df, frac=0.25):\n",
    "    # get random sample \n",
    "    test = df.sample(frac=frac, axis=0)\n",
    "    # get everything but the test sample\n",
    "    train = df.drop(index=test.index)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "df = open_csv_clustered()\n",
    "df_data, df_test = train_test_split(df)\n",
    "formatted_data = df_data.values.tolist()\n",
    "formatted_test = df_test.values.tolist()\n",
    "jobs, skills = list(zip(*formatted_test))\n",
    "skills = [ast.literal_eval(skill) for skill in skills]\n",
    "\n",
    "# Option 1: Doc2Vec\n",
    "model_option1 = trainDoc2Vec(formatted_data)\n",
    "\n",
    "# Option 2: Word2Vec\n",
    "job_averages_option2, keys_option2, values_option2, model_option2 = prepareOption2(formatted_data)\n",
    "\n",
    "# Option 3: Doc2Vec Embedding\n",
    "job_averages_option3, keys_option3, values_option3, model_option3 = prepareOption3(formatted_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13f09ce-bce0-4dd5-b65d-49fb78b5b768",
   "metadata": {},
   "source": [
    "# 4. Testing für Option 1, 2 und 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e5db242-b1e1-477b-8614-9fae142ecc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 1 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match: [1, 7, 0]\n",
      "\n",
      "Option 2 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match: [0, 7, 1]\n",
      "\n",
      "Option 3 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match: [7, 0, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "skills_example = ['azure', 'azure', 'streaming', 'big_data']\n",
    "\n",
    "# Option 1: Doc2Vec\n",
    "infer_vector_option1 = model_option1.infer_vector(skills_example)\n",
    "similar_jobs_option1 = model_option1.dv.most_similar([infer_vector_option1], topn=3)\n",
    "predicted_jobs_option1 = [job for job, similarity in similar_jobs_option1]\n",
    "print(\"Option 1 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match:\", predicted_jobs_option1)\n",
    "print(\"\")\n",
    "\n",
    "# Option 2: Word2Vec\n",
    "avg_option2 = averageOfSkills(model_option2, skills_example)\n",
    "distVec_option2 = cdist([avg_option2], values_option2)\n",
    "topJobs_option2 = [k for dist, k in heapq.nsmallest(3, zip(distVec_option2.transpose(), keys_option2))]\n",
    "print(\"Option 2 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match:\", topJobs_option2)\n",
    "print(\"\")\n",
    "\n",
    "# Option 3: Doc2Vec Embedding\n",
    "avg_option3 = averageOfSkills(model_option3, skills_example)\n",
    "distVec_option3 = cdist([avg_option3], values_option3)\n",
    "topJobs_option3 = [k for dist, k in heapq.nsmallest(3, zip(distVec_option3.transpose(), keys_option3))]\n",
    "print(\"Option 3 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match:\", topJobs_option3)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8064fd-8134-48ef-9e4e-3087907e3987",
   "metadata": {},
   "source": [
    "# 5. Model evaluation für Option 1, 2 und 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "LA8npAnMSkW6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LA8npAnMSkW6",
    "outputId": "ed793db2-71c2-4631-d83c-eb1b0cbed214"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn the following, I will try to calculate:\\n•\\tCondition positive (P): The number of real positive cases in the data.\\n•\\tCondition negative (N): The number of real negative cases in the data.\\n•\\tTrue positive (TP): A test result that correctly indicates the presence of a condition or characteristic.\\n•\\tTrue negative (TN): A test result that correctly indicates the absence of a condition or characteristic.\\n•\\tFalse positive (FP): A test result that wrongly indicates that a particular condition or attribute is present.\\n•\\tFalse negative (FN): A test result that wrongly indicates that a particular condition or attribute is absent.\\n•\\tSensitivity, recall, hit rate, or true positive rate (TPR): The proportion of true positives out of all actual positive cases.\\n•\\tSpecificity, selectivity, or true negative rate (TNR): The proportion of true negatives out of all actual negative cases.\\n•\\tPrecision or positive predictive value (PPV): The proportion of true positives out of all positive predictions made by the model.\\n•\\tNegative predictive value (NPV): The proportion of true negatives out of all negative predictions made by the model.\\n•\\tMiss rate or false negative rate (FNR): The proportion of false negatives out of all actual positive cases.\\n•\\tFall-out or false positive rate (FPR): The proportion of false positives out of all actual negative cases.\\n•\\tFalse discovery rate (FDR): The proportion of false positives out of all positive predictions made by the model.\\n•\\tFalse omission rate (FOR): The proportion of false negatives out of all negative predictions made by the model.\\n•\\tPositive likelihood ratio (LR+): The ratio of true positive rate to false positive rate.\\n•\\tNegative likelihood ratio (LR-): The ratio of false negative rate to true negative rate.\\n•\\tPrevalence threshold (PT): A threshold value that balances sensitivity and specificity.\\n•\\tThreat score (TS) or critical success index (CSI): The proportion of true positives out of all positive and negative predictions made by the model.\\n•\\tPrevalence: The proportion of positive cases in the data.\\n•\\tAccuracy (ACC): The proportion of correct predictions (both true positives and true negatives) out of all predictions made by the model.\\n•\\tBalanced accuracy (BA): The average of sensitivity and specificity.\\n•\\tF1 score: The harmonic mean of precision and sensitivity, providing a single metric to balance both.\\n•\\tPhi coefficient (φ) or Matthews correlation coefficient (MCC): A correlation coefficient that takes into account true positives, true negatives, false positives, and false negatives.\\n•\\tFowlkes-Mallows index (FM): A geometric mean of precision and sensitivity.\\n•\\tInformedness or bookmaker informedness (BM): The sum of true positive rate and true negative rate, minus 1.\\n•\\tMarkedness (MK) or deltaP (Δp): The sum of positive predictive value and negative predictive value, minus 1.\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation and assessment of binary classification models\n",
    "\n",
    "'''\n",
    "In the following, I will try to calculate:\n",
    "•\tCondition positive (P): The number of real positive cases in the data.\n",
    "•\tCondition negative (N): The number of real negative cases in the data.\n",
    "•\tTrue positive (TP): A test result that correctly indicates the presence of a condition or characteristic.\n",
    "•\tTrue negative (TN): A test result that correctly indicates the absence of a condition or characteristic.\n",
    "•\tFalse positive (FP): A test result that wrongly indicates that a particular condition or attribute is present.\n",
    "•\tFalse negative (FN): A test result that wrongly indicates that a particular condition or attribute is absent.\n",
    "•\tSensitivity, recall, hit rate, or true positive rate (TPR): The proportion of true positives out of all actual positive cases.\n",
    "•\tSpecificity, selectivity, or true negative rate (TNR): The proportion of true negatives out of all actual negative cases.\n",
    "•\tPrecision or positive predictive value (PPV): The proportion of true positives out of all positive predictions made by the model.\n",
    "•\tNegative predictive value (NPV): The proportion of true negatives out of all negative predictions made by the model.\n",
    "•\tMiss rate or false negative rate (FNR): The proportion of false negatives out of all actual positive cases.\n",
    "•\tFall-out or false positive rate (FPR): The proportion of false positives out of all actual negative cases.\n",
    "•\tFalse discovery rate (FDR): The proportion of false positives out of all positive predictions made by the model.\n",
    "•\tFalse omission rate (FOR): The proportion of false negatives out of all negative predictions made by the model.\n",
    "•\tPositive likelihood ratio (LR+): The ratio of true positive rate to false positive rate.\n",
    "•\tNegative likelihood ratio (LR-): The ratio of false negative rate to true negative rate.\n",
    "•\tPrevalence threshold (PT): A threshold value that balances sensitivity and specificity.\n",
    "•\tThreat score (TS) or critical success index (CSI): The proportion of true positives out of all positive and negative predictions made by the model.\n",
    "•\tPrevalence: The proportion of positive cases in the data.\n",
    "•\tAccuracy (ACC): The proportion of correct predictions (both true positives and true negatives) out of all predictions made by the model.\n",
    "•\tBalanced accuracy (BA): The average of sensitivity and specificity.\n",
    "•\tF1 score: The harmonic mean of precision and sensitivity, providing a single metric to balance both.\n",
    "•\tPhi coefficient (φ) or Matthews correlation coefficient (MCC): A correlation coefficient that takes into account true positives, true negatives, false positives, and false negatives.\n",
    "•\tFowlkes-Mallows index (FM): A geometric mean of precision and sensitivity.\n",
    "•\tInformedness or bookmaker informedness (BM): The sum of true positive rate and true negative rate, minus 1.\n",
    "•\tMarkedness (MK) or deltaP (Δp): The sum of positive predictive value and negative predictive value, minus 1.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a696291-2b34-44ce-919a-3bb1b3606c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground_truth_labels = [job[0] for job in formatted_test]\n",
    "# print(ground_truth_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "328836ca-2048-4a5a-bc28-bd5a02e16573",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "328836ca-2048-4a5a-bc28-bd5a02e16573",
    "outputId": "6dee34cc-b8b9-4c5a-99be-f450c3b514f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 1:\n",
      "True Positives (TP): 3\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 2763\n",
      "True Negatives (TN): 0\n",
      "\n",
      "Option 2:\n",
      "True Positives (TP): 3\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 2763\n",
      "True Negatives (TN): 0\n",
      "\n",
      "Option 3:\n",
      "True Positives (TP): 3\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 2763\n",
      "True Negatives (TN): 0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• True positive (TP): A test result that correctly indicates the presence of a condition or characteristic.\n",
    "• True negative (TN): A test result that correctly indicates the absence of a condition or characteristic.\n",
    "• False positive (FP): A test result that wrongly indicates that a particular condition or attribute is present.\n",
    "• False negative (FN): A test result that wrongly indicates that a particular condition or attribute is absent.\n",
    "\n",
    "'''\n",
    "# job[0] is the jo\n",
    "ground_truth_labels = [job[0] for job in formatted_test]\n",
    "\n",
    "\n",
    "# Calculate the total number of samples in your dataset\n",
    "total_samples = len(df_test)\n",
    "\n",
    "# Option 1\n",
    "tp_option1 = len(set(predicted_jobs_option1) & set(ground_truth_labels))\n",
    "fp_option1 = len(predicted_jobs_option1) - tp_option1\n",
    "fn_option1 = len(ground_truth_labels) - tp_option1\n",
    "tn_option1 = total_samples - (tp_option1 + fp_option1 + fn_option1)\n",
    "\n",
    "print(\"Option 1:\")\n",
    "print(\"True Positives (TP):\", tp_option1)\n",
    "print(\"False Positives (FP):\", fp_option1)\n",
    "print(\"False Negatives (FN):\", fn_option1)\n",
    "print(\"True Negatives (TN):\", tn_option1)\n",
    "\n",
    "# Option 2\n",
    "tp_option2 = len(set(topJobs_option2) & set(ground_truth_labels))\n",
    "fp_option2 = len(topJobs_option2) - tp_option2\n",
    "fn_option2 = len(ground_truth_labels) - tp_option2\n",
    "tn_option2 = total_samples - (tp_option2 + fp_option2 + fn_option2)\n",
    "\n",
    "print(\"\\nOption 2:\")\n",
    "print(\"True Positives (TP):\", tp_option2)\n",
    "print(\"False Positives (FP):\", fp_option2)\n",
    "print(\"False Negatives (FN):\", fn_option2)\n",
    "print(\"True Negatives (TN):\", tn_option2)\n",
    "\n",
    "# Option 3\n",
    "tp_option3 = len(set(topJobs_option3) & set(ground_truth_labels))\n",
    "fp_option3 = len(topJobs_option3) - tp_option3\n",
    "fn_option3 = len(ground_truth_labels) - tp_option3\n",
    "tn_option3 = total_samples - (tp_option3 + fp_option3 + fn_option3)\n",
    "\n",
    "print(\"\\nOption 3:\")\n",
    "print(\"True Positives (TP):\", tp_option3)\n",
    "print(\"False Positives (FP):\", fp_option3)\n",
    "print(\"False Negatives (FN):\", fn_option3)\n",
    "print(\"True Negatives (TN):\", tn_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de62110e-d19b-435c-ac3d-304ccf24742b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity (TPR) - Option 1: 0.0010845986984815619\n",
      "Sensitivity (TPR) - Option 2: 0.0010845986984815619\n",
      "Sensitivity (TPR) - Option 3: 0.0010845986984815619\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• Sensitivity, recall, hit rate, or true positive rate (TPR): The proportion of true positives out of all actual positive cases.\n",
    "'''\n",
    "\n",
    "# Calculate Sensitivity (TPR) for Option 1\n",
    "sensitivity_option1 = tp_option1 / (tp_option1 + fn_option1)\n",
    "\n",
    "# Calculate Sensitivity (TPR) for Option 2\n",
    "sensitivity_option2 = tp_option2 / (tp_option2 + fn_option2)\n",
    "\n",
    "# Calculate Sensitivity (TPR) for Option 3\n",
    "sensitivity_option3 = tp_option3 / (tp_option3 + fn_option3)\n",
    "\n",
    "print(\"Sensitivity (TPR) - Option 1:\", sensitivity_option1)\n",
    "print(\"Sensitivity (TPR) - Option 2:\", sensitivity_option2)\n",
    "print(\"Sensitivity (TPR) - Option 3:\", sensitivity_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed2e5ad6-ba1e-44b5-bf1d-54d768a40fbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12012/2770829557.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m '''\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Calculate Specificity (TNR) for Option 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mspecificity_option1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtn_option1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtn_option1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfp_option1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Calculate Specificity (TNR) for Option 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• Specificity, selectivity, or true negative rate (TNR): The proportion of true negatives out of all actual negative cases.\n",
    "'''\n",
    "# Calculate Specificity (TNR) for Option 1\n",
    "specificity_option1 = tn_option1 / (tn_option1 + fp_option1)\n",
    "\n",
    "# Calculate Specificity (TNR) for Option 2\n",
    "specificity_option2 = tn_option2 / (tn_option2 + fp_option2)\n",
    "\n",
    "# Calculate Specificity (TNR) for Option 3\n",
    "specificity_option3 = tn_option3 / (tn_option3 + fp_option3)\n",
    "\n",
    "print(\"Specificity (TNR) - Option 1:\", specificity_option1)\n",
    "print(\"Specificity (TNR) - Option 2:\", specificity_option2)\n",
    "print(\"Specificity (TNR) - Option 3:\", specificity_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b16f6e9a-bb78-436b-89f8-366851c4aaf0",
   "metadata": {
    "id": "b16f6e9a-bb78-436b-89f8-366851c4aaf0",
    "outputId": "9658cf1e-f519-4bff-9483-81e3e9fe6f36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (PPV) - Option 1: 1.0\n",
      "Precision (PPV) - Option 2: 1.0\n",
      "Precision (PPV) - Option 3: 1.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• Precision or positive predictive value (PPV): The proportion of true positives out of all positive predictions made by the model.\n",
    "'''\n",
    "\n",
    "# Calculate Precision (PPV) for Option 1\n",
    "precision_option1 = tp_option1 / (tp_option1 + fp_option1)\n",
    "\n",
    "# Calculate Precision (PPV) for Option 2\n",
    "precision_option2 = tp_option2 / (tp_option2 + fp_option2)\n",
    "\n",
    "# Calculate Precision (PPV) for Option 3\n",
    "precision_option3 = tp_option3 / (tp_option3 + fp_option3)\n",
    "\n",
    "print(\"Precision (PPV) - Option 1:\", precision_option1)\n",
    "print(\"Precision (PPV) - Option 2:\", precision_option2)\n",
    "print(\"Precision (PPV) - Option 3:\", precision_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ef591a1-0d98-4da9-9c8a-4e7c6e68dcf3",
   "metadata": {
    "id": "5ef591a1-0d98-4da9-9c8a-4e7c6e68dcf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Predictive Value (NPV) - Option 1: 0.0\n",
      "Negative Predictive Value (NPV) - Option 2: 0.0\n",
      "Negative Predictive Value (NPV) - Option 3: 0.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• Negative predictive value (NPV): The proportion of true negatives out of all negative predictions made by the model.\n",
    "'''\n",
    "\n",
    "# Calculate Negative Predictive Value (NPV) for Option 1\n",
    "npv_option1 = tn_option1 / (tn_option1 + fn_option1)\n",
    "\n",
    "# Calculate Negative Predictive Value (NPV) for Option 2\n",
    "npv_option2 = tn_option2 / (tn_option2 + fn_option2)\n",
    "\n",
    "# Calculate Negative Predictive Value (NPV) for Option 3\n",
    "npv_option3 = tn_option3 / (tn_option3 + fn_option3)\n",
    "\n",
    "print(\"Negative Predictive Value (NPV) - Option 1:\", npv_option1)\n",
    "print(\"Negative Predictive Value (NPV) - Option 2:\", npv_option2)\n",
    "print(\"Negative Predictive Value (NPV) - Option 3:\", npv_option3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af114b38-fc42-4537-b9dd-b036fb856e52",
   "metadata": {
    "id": "af114b38-fc42-4537-b9dd-b036fb856e52",
    "outputId": "68867287-7d87-47ef-bbbc-4d6c9927034a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negative Rate (FNR) - Option 1: 0.9989154013015185\n",
      "False Negative Rate (FNR) - Option 2: 0.9989154013015185\n",
      "False Negative Rate (FNR) - Option 3: 0.9989154013015185\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• Miss rate or false negative rate (FNR): The proportion of false negatives out of all actual positive cases.\n",
    "'''\n",
    "\n",
    "# Calculate False Negative Rate (FNR) for Option 1\n",
    "fnr_option1 = fn_option1 / (fn_option1 + tp_option1)\n",
    "\n",
    "# Calculate False Negative Rate (FNR) for Option 2\n",
    "fnr_option2 = fn_option2 / (fn_option2 + tp_option2)\n",
    "\n",
    "# Calculate False Negative Rate (FNR) for Option 3\n",
    "fnr_option3 = fn_option3 / (fn_option3 + tp_option3)\n",
    "\n",
    "print(\"False Negative Rate (FNR) - Option 1:\", fnr_option1)\n",
    "print(\"False Negative Rate (FNR) - Option 2:\", fnr_option2)\n",
    "print(\"False Negative Rate (FNR) - Option 3:\", fnr_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22299028-f4a9-4028-9137-0b3602102396",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12012/2433927177.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m '''\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Calculate False Positive Rate (FPR) for Option 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfpr_option1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp_option1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfp_option1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtn_option1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Calculate False Positive Rate (FPR) for Option 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• Fall-out or false positive rate (FPR): The proportion of false positives out of all actual negative cases.\n",
    "'''\n",
    "# Calculate False Positive Rate (FPR) for Option 1\n",
    "fpr_option1 = fp_option1 / (fp_option1 + tn_option1)\n",
    "\n",
    "# Calculate False Positive Rate (FPR) for Option 2\n",
    "fpr_option2 = fp_option2 / (fp_option2 + tn_option2)\n",
    "\n",
    "# Calculate False Positive Rate (FPR) for Option 3\n",
    "fpr_option3 = fp_option3 / (fp_option3 + tn_option3)\n",
    "\n",
    "print(\"False Positive Rate (FPR) - Option 1:\", fpr_option1)\n",
    "print(\"False Positive Rate (FPR) - Option 2:\", fpr_option2)\n",
    "print(\"False Positive Rate (FPR) - Option 3:\", fpr_option3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e26fe6d8-340a-4377-abf1-6c20ffd53854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Discovery Rate (FDR) - Option 1: 0.0\n",
      "False Discovery Rate (FDR) - Option 2: 0.0\n",
      "False Discovery Rate (FDR) - Option 3: 0.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• False discovery rate (FDR): The proportion of false positives out of all positive predictions made by the model.\n",
    "'''\n",
    "# Calculate False Discovery Rate (FDR) for Option 1\n",
    "fdr_option1 = fp_option1 / (fp_option1 + tp_option1)\n",
    "\n",
    "# Calculate False Discovery Rate (FDR) for Option 2\n",
    "fdr_option2 = fp_option2 / (fp_option2 + tp_option2)\n",
    "\n",
    "# Calculate False Discovery Rate (FDR) for Option 3\n",
    "fdr_option3 = fp_option3 / (fp_option3 + tp_option3)\n",
    "\n",
    "print(\"False Discovery Rate (FDR) - Option 1:\", fdr_option1)\n",
    "print(\"False Discovery Rate (FDR) - Option 2:\", fdr_option2)\n",
    "print(\"False Discovery Rate (FDR) - Option 3:\", fdr_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2d54d52-43ea-4a93-b19c-9cd80519456c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Omission Rate (FOR) - Option 1: 1.0\n",
      "False Omission Rate (FOR) - Option 2: 1.0\n",
      "False Omission Rate (FOR) - Option 3: 1.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• False omission rate (FOR): The proportion of false negatives out of all negative predictions made by the model.\n",
    "'''\n",
    "# Calculate False Omission Rate (FOR) for Option 1\n",
    "for_option1 = fn_option1 / (fn_option1 + tn_option1)\n",
    "\n",
    "# Calculate False Omission Rate (FOR) for Option 2\n",
    "for_option2 = fn_option2 / (fn_option2 + tn_option2)\n",
    "\n",
    "# Calculate False Omission Rate (FOR) for Option 3\n",
    "for_option3 = fn_option3 / (fn_option3 + tn_option3)\n",
    "\n",
    "print(\"False Omission Rate (FOR) - Option 1:\", for_option1)\n",
    "print(\"False Omission Rate (FOR) - Option 2:\", for_option2)\n",
    "print(\"False Omission Rate (FOR) - Option 3:\", for_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d768ac07-66bf-4079-9fcb-592609ce98cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12012/2259375502.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Calculate False Positive Rate (FPR) for Option 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mfpr_option1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp_option1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfp_option1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtn_option1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Calculate True Positive Rate (TPR) for Option 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• Positive likelihood ratio (LR+): The ratio of true positive rate to false positive rate.\n",
    "'''\n",
    "\n",
    "# Calculate True Positive Rate (TPR) for Option 1\n",
    "tpr_option1 = tp_option1 / (tp_option1 + fn_option1)\n",
    "\n",
    "# Calculate False Positive Rate (FPR) for Option 1\n",
    "fpr_option1 = fp_option1 / (fp_option1 + tn_option1)\n",
    "\n",
    "# Calculate True Positive Rate (TPR) for Option 2\n",
    "tpr_option2 = tp_option2 / (tp_option2 + fn_option2)\n",
    "\n",
    "# Calculate False Positive Rate (FPR) for Option 2\n",
    "fpr_option2 = fp_option2 / (fp_option2 + tn_option2)\n",
    "\n",
    "# Calculate True Positive Rate (TPR) for Option 3\n",
    "tpr_option3 = tp_option3 / (tp_option3 + fn_option3)\n",
    "\n",
    "# Calculate False Positive Rate (FPR) for Option 3\n",
    "fpr_option3 = fp_option3 / (fp_option3 + tn_option3)\n",
    "\n",
    "# Calculate Positive Likelihood Ratio (LR+) for Option 1\n",
    "lr_plus_option1 = tpr_option1 / fpr_option1\n",
    "\n",
    "# Calculate Positive Likelihood Ratio (LR+) for Option 2\n",
    "lr_plus_option2 = tpr_option2 / fpr_option2\n",
    "\n",
    "# Calculate Positive Likelihood Ratio (LR+) for Option 3\n",
    "lr_plus_option3 = tpr_option3 / fpr_option3\n",
    "\n",
    "print(\"Positive Likelihood Ratio (LR+) - Option 1:\", lr_plus_option1)\n",
    "print(\"Positive Likelihood Ratio (LR+) - Option 2:\", lr_plus_option2)\n",
    "print(\"Positive Likelihood Ratio (LR+) - Option 3:\", lr_plus_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2589984a-4e4f-4995-afdb-6234b5408ce3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12012/2401965536.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Calculate True Negative Rate (TNR) for Option 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtnr_option1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtn_option1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtn_option1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfp_option1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Calculate False Negative Rate (FNR) for Option 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• Negative likelihood ratio (LR-): The ratio of false negative rate to true negative rate.\n",
    "'''\n",
    "# Calculate False Negative Rate (FNR) for Option 1\n",
    "fnr_option1 = fn_option1 / (fn_option1 + tp_option1)\n",
    "\n",
    "# Calculate True Negative Rate (TNR) for Option 1\n",
    "tnr_option1 = tn_option1 / (tn_option1 + fp_option1)\n",
    "\n",
    "# Calculate False Negative Rate (FNR) for Option 2\n",
    "fnr_option2 = fn_option2 / (fn_option2 + tp_option2)\n",
    "\n",
    "# Calculate True Negative Rate (TNR) for Option 2\n",
    "tnr_option2 = tn_option2 / (tn_option2 + fp_option2)\n",
    "\n",
    "# Calculate False Negative Rate (FNR) for Option 3\n",
    "fnr_option3 = fn_option3 / (fn_option3 + tp_option3)\n",
    "\n",
    "# Calculate True Negative Rate (TNR) for Option 3\n",
    "tnr_option3 = tn_option3 / (tn_option3 + fp_option3)\n",
    "\n",
    "# Calculate Negative Likelihood Ratio (LR-) for Option 1\n",
    "lr_minus_option1 = fnr_option1 / tnr_option1\n",
    "\n",
    "# Calculate Negative Likelihood Ratio (LR-) for Option 2\n",
    "lr_minus_option2 = fnr_option2 / tnr_option2\n",
    "\n",
    "# Calculate Negative Likelihood Ratio (LR-) for Option 3\n",
    "lr_minus_option3 = fnr_option3 / tnr_option3\n",
    "\n",
    "print(\"Negative Likelihood Ratio (LR-) - Option 1:\", lr_minus_option1)\n",
    "print(\"Negative Likelihood Ratio (LR-) - Option 2:\", lr_minus_option2)\n",
    "print(\"Negative Likelihood Ratio (LR-) - Option 3:\", lr_minus_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0193a6fd-616d-429b-895d-d5904a664438",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12012/2689190115.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Calculate TPR and TNR for Option 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtpr_option1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp_option1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtp_option1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfn_option1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtnr_option1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtn_option1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtn_option1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfp_option1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Calculate Prevalence Threshold (PT) for Option 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• Prevalence threshold (PT): A threshold value that balances sensitivity and specificity.\n",
    "'''\n",
    "# Calculate TPR and TNR for Option 1\n",
    "tpr_option1 = tp_option1 / (tp_option1 + fn_option1)\n",
    "tnr_option1 = tn_option1 / (tn_option1 + fp_option1)\n",
    "\n",
    "# Calculate Prevalence Threshold (PT) for Option 1\n",
    "pt_option1 = (math.sqrt(tpr_option1 * (1 - tnr_option1)) + tnr_option1 - 1) / (tpr_option1 + tnr_option1 - 1)\n",
    "\n",
    "print(\"Prevalence Threshold (PT) - Option 1:\", pt_option1)\n",
    "\n",
    "# Calculate TPR and TNR for Option 2\n",
    "tpr_option2 = tp_option2 / (tp_option2 + fn_option2)\n",
    "tnr_option2 = tn_option2 / (tn_option2 + fp_option2)\n",
    "\n",
    "# Calculate Prevalence Threshold (PT) for Option 2\n",
    "pt_option2 = (math.sqrt(tpr_option2 * (1 - tnr_option2)) + tnr_option2 - 1) / (tpr_option2 + tnr_option2 - 1)\n",
    "\n",
    "print(\"Prevalence Threshold (PT) - Option 2:\", pt_option2)\n",
    "\n",
    "# Calculate TPR and TNR for Option 3\n",
    "tpr_option3 = tp_option3 / (tp_option3 + fn_option3)\n",
    "tnr_option3 = tn_option3 / (tn_option3 + fp_option3)\n",
    "\n",
    "# Calculate Prevalence Threshold (PT) for Option 3\n",
    "pt_option3 = (math.sqrt(tpr_option3 * (1 - tnr_option3)) + tnr_option3 - 1) / (tpr_option3 + tnr_option3 - 1)\n",
    "\n",
    "print(\"Prevalence Threshold (PT) - Option 3:\", pt_option3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b6c02df-0795-4c31-bfb7-7f0fd98c967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threat Score (TS) - Option 1: 0.0010845986984815619\n",
      "Threat Score (TS) - Option 2: 0.0010845986984815619\n",
      "Threat Score (TS) - Option 3: 0.0010845986984815619\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• Threat score (TS) or critical success index (CSI): The proportion of true positives out of all positive and negative predictions made by the model.\n",
    "'''\n",
    "\n",
    "# Calculate Threat Score (TS) / Critical Success Index (CSI) for Option 1\n",
    "ts_option1 = tp_option1 / (tp_option1 + fp_option1 + fn_option1)\n",
    "\n",
    "print(\"Threat Score (TS) - Option 1:\", ts_option1)\n",
    "\n",
    "# Calculate Threat Score (TS) / Critical Success Index (CSI) for Option 2\n",
    "ts_option2 = tp_option2 / (tp_option2 + fp_option2 + fn_option2)\n",
    "\n",
    "print(\"Threat Score (TS) - Option 2:\", ts_option2)\n",
    "\n",
    "# Calculate Threat Score (TS) / Critical Success Index (CSI) for Option 3\n",
    "ts_option3 = tp_option3 / (tp_option3 + fp_option3 + fn_option3)\n",
    "\n",
    "print(\"Threat Score (TS) - Option 3:\", ts_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "73bb4611-258f-4677-8a7f-875f55ddef4b",
   "metadata": {
    "id": "73bb4611-258f-4677-8a7f-875f55ddef4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prevalence - Option 1: 0.0010845986984815619\n",
      "Prevalence - Option 2: 0.0010845986984815619\n",
      "Prevalence - Option 3: 0.0010845986984815619\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• Prevalence: The proportion of positive cases in the data.\n",
    "'''\n",
    "# Calculate Prevalence for Option 1\n",
    "prevalence_option1 = tp_option1 / total_samples\n",
    "\n",
    "print(\"Prevalence - Option 1:\", prevalence_option1)\n",
    "\n",
    "# Calculate Prevalence for Option 2\n",
    "prevalence_option2 = tp_option2 / total_samples\n",
    "\n",
    "print(\"Prevalence - Option 2:\", prevalence_option2)\n",
    "\n",
    "# Calculate Prevalence for Option 3\n",
    "prevalence_option3 = tp_option3 / total_samples\n",
    "\n",
    "print(\"Prevalence - Option 3:\", prevalence_option3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f01bb76c-037e-476b-be33-8591222bd8b9",
   "metadata": {
    "id": "f01bb76c-037e-476b-be33-8591222bd8b9",
    "outputId": "4671c668-0d2b-4887-a761-aacd6688a4d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - Option 1: 0.0010845986984815619\n",
      "Accuracy - Option 2: 0.0010845986984815619\n",
      "Accuracy - Option 3: 0.0010845986984815619\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• Accuracy (ACC): The proportion of correct predictions (both true positives and true negatives) out of all predictions made by the model.\n",
    "'''\n",
    "\n",
    "# Calculate Accuracy for Option 1\n",
    "accuracy_option1 = (tp_option1 + tn_option1) / total_samples\n",
    "\n",
    "print(\"Accuracy - Option 1:\", accuracy_option1)\n",
    "\n",
    "# Calculate Accuracy for Option 2\n",
    "accuracy_option2 = (tp_option2 + tn_option2) / total_samples\n",
    "\n",
    "print(\"Accuracy - Option 2:\", accuracy_option2)\n",
    "\n",
    "# Calculate Accuracy for Option 3\n",
    "accuracy_option3 = (tp_option3 + tn_option3) / total_samples\n",
    "\n",
    "print(\"Accuracy - Option 3:\", accuracy_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be6cdf5d-ed5b-43b2-945b-383f4421b2b6",
   "metadata": {
    "id": "be6cdf5d-ed5b-43b2-945b-383f4421b2b6"
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12012/3360806120.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m '''\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Calculate Specificity (TNR) for each option\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtnr_option1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtn_option1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtn_option1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfp_option1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtnr_option2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtn_option2\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtn_option2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfp_option2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtnr_option3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtn_option3\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtn_option3\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfp_option3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• Balanced accuracy (BA): The average of sensitivity and specificity.\n",
    "'''\n",
    "# Calculate Specificity (TNR) for each option\n",
    "tnr_option1 = tn_option1 / (tn_option1 + fp_option1)\n",
    "tnr_option2 = tn_option2 / (tn_option2 + fp_option2)\n",
    "tnr_option3 = tn_option3 / (tn_option3 + fp_option3)\n",
    "\n",
    "# Calculate Balanced Accuracy for Option 1\n",
    "ba_option1 = (tpr_option1 + tnr_option1) / 2\n",
    "\n",
    "print(\"Balanced Accuracy - Option 1:\", ba_option1)\n",
    "\n",
    "# Calculate Balanced Accuracy for Option 2\n",
    "ba_option2 = (tpr_option2 + tnr_option2) / 2\n",
    "\n",
    "print(\"Balanced Accuracy - Option 2:\", ba_option2)\n",
    "\n",
    "# Calculate Balanced Accuracy for Option 3\n",
    "ba_option3 = (tpr_option3 + tnr_option3) / 2\n",
    "\n",
    "print(\"Balanced Accuracy - Option 3:\", ba_option3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "175ad77b-4f27-4131-aa9f-0522b1cf6280",
   "metadata": {
    "id": "175ad77b-4f27-4131-aa9f-0522b1cf6280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score - Option 1: 0.002166847237269773\n",
      "F1 Score - Option 2: 0.002166847237269773\n",
      "F1 Score - Option 3: 0.002166847237269773\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• F1 score: The harmonic mean of precision and sensitivity, providing a single metric to balance both.\n",
    "'''\n",
    "\n",
    "# Calculate F1 Score for Option 1\n",
    "precision_option1 = tp_option1 / (tp_option1 + fp_option1)\n",
    "recall_option1 = tp_option1 / (tp_option1 + fn_option1)\n",
    "f1_score_option1 = 2 * (precision_option1 * recall_option1) / (precision_option1 + recall_option1)\n",
    "\n",
    "print(\"F1 Score - Option 1:\", f1_score_option1)\n",
    "\n",
    "# Calculate F1 Score for Option 2\n",
    "precision_option2 = tp_option2 / (tp_option2 + fp_option2)\n",
    "recall_option2 = tp_option2 / (tp_option2 + fn_option2)\n",
    "f1_score_option2 = 2 * (precision_option2 * recall_option2) / (precision_option2 + recall_option2)\n",
    "\n",
    "print(\"F1 Score - Option 2:\", f1_score_option2)\n",
    "\n",
    "# Calculate F1 Score for Option 3\n",
    "precision_option3 = tp_option3 / (tp_option3 + fp_option3)\n",
    "recall_option3 = tp_option3 / (tp_option3 + fn_option3)\n",
    "f1_score_option3 = 2 * (precision_option3 * recall_option3) / (precision_option3 + recall_option3)\n",
    "\n",
    "print(\"F1 Score - Option 3:\", f1_score_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "695c0457-9e97-485c-b125-1d0a80e6b7e6",
   "metadata": {
    "id": "695c0457-9e97-485c-b125-1d0a80e6b7e6",
    "outputId": "8c1576ae-e06e-4d10-befc-fd95b2ffdc77"
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12012/3211412553.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Calculate Phi coefficient (MCC) for Option 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmcc_option1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtp_option1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtn_option1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfp_option1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfn_option1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp_option1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfp_option1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtp_option1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfn_option1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtn_option1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfp_option1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtn_option1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfn_option1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Phi Coefficient (MCC) - Option 1:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmcc_option1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• Phi coefficient (φ) or Matthews correlation coefficient (MCC): A correlation coefficient that takes into account true positives, true negatives, false positives, and false negatives.\n",
    "'''\n",
    "\n",
    "# Calculate Phi coefficient (MCC) for Option 1\n",
    "mcc_option1 = (tp_option1 * tn_option1 - fp_option1 * fn_option1) / ((tp_option1 + fp_option1) * (tp_option1 + fn_option1) * (tn_option1 + fp_option1) * (tn_option1 + fn_option1)) ** 0.5\n",
    "\n",
    "print(\"Phi Coefficient (MCC) - Option 1:\", mcc_option1)\n",
    "\n",
    "# Calculate Phi coefficient (MCC) for Option 2\n",
    "mcc_option2 = (tp_option2 * tn_option2 - fp_option2 * fn_option2) / ((tp_option2 + fp_option2) * (tp_option2 + fn_option2) * (tn_option2 + fp_option2) * (tn_option2 + fn_option2)) ** 0.5\n",
    "\n",
    "print(\"Phi Coefficient (MCC) - Option 2:\", mcc_option2)\n",
    "\n",
    "# Calculate Phi coefficient (MCC) for Option 3\n",
    "mcc_option3 = (tp_option3 * tn_option3 - fp_option3 * fn_option3) / ((tp_option3 + fp_option3) * (tp_option3 + fn_option3) * (tn_option3 + fp_option3) * (tn_option3 + fn_option3)) ** 0.5\n",
    "\n",
    "print(\"Phi Coefficient (MCC) - Option 3:\", mcc_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5db0488-11a9-49fe-9b7c-94227a7727e7",
   "metadata": {
    "id": "e5db0488-11a9-49fe-9b7c-94227a7727e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fowlkes-Mallows index (FM) - Option 1: 0.03293324609693921\n",
      "Fowlkes-Mallows index (FM) - Option 2: 0.03293324609693921\n",
      "Fowlkes-Mallows index (FM) - Option 3: 0.03293324609693921\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• Fowlkes-Mallows index (FM): A geometric mean of precision and sensitivity.\n",
    "'''\n",
    "# Calculate Fowlkes-Mallows index (FM) for Option 1\n",
    "fm_option1 = (tp_option1 * tp_option1 / ((tp_option1 + fp_option1) * (tp_option1 + fn_option1))) ** 0.5\n",
    "\n",
    "print(\"Fowlkes-Mallows index (FM) - Option 1:\", fm_option1)\n",
    "\n",
    "# Calculate Fowlkes-Mallows index (FM) for Option 2\n",
    "fm_option2 = (tp_option2 * tp_option2 / ((tp_option2 + fp_option2) * (tp_option2 + fn_option2))) ** 0.5\n",
    "\n",
    "print(\"Fowlkes-Mallows index (FM) - Option 2:\", fm_option2)\n",
    "\n",
    "# Calculate Fowlkes-Mallows index (FM) for Option 3\n",
    "fm_option3 = (tp_option3 * tp_option3 / ((tp_option3 + fp_option3) * (tp_option3 + fn_option3))) ** 0.5\n",
    "\n",
    "print(\"Fowlkes-Mallows index (FM) - Option 3:\", fm_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ab1308-e5c4-4f69-b114-9d0d107b2e19",
   "metadata": {
    "id": "65ab1308-e5c4-4f69-b114-9d0d107b2e19"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b31c35-8bae-40a3-87a5-6c94732b05e7",
   "metadata": {
    "id": "13b31c35-8bae-40a3-87a5-6c94732b05e7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a6ead-8519-4bd5-89f0-d05b0ec21d4b",
   "metadata": {
    "id": "d11a6ead-8519-4bd5-89f0-d05b0ec21d4b"
   },
   "outputs": [],
   "source": [
    "•\tInformedness or bookmaker informedness (BM): The sum of true positive rate and true negative rate, minus 1.\n",
    "•\tMarkedness (MK) or deltaP (Δp): The sum of positive predictive value and negative predictive value, minus 1."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
