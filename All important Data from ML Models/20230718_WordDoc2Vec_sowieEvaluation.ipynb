{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a74965-7c85-4d75-be00-9068d7a967dd",
   "metadata": {},
   "source": [
    "# Modelle (Word2Vec, Doc2Vec) sowie Evaluation aller Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e7030c-3a8c-4a74-b549-87489393fda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Datenaufbereitung\\n2. Methodendeklaration, Hilfsfunktion, unsw.\\n3. Training (Word2Vec, Doc2Vec)\\n4. Laden der Training-Ergebnisse von LinkPred, Knowledge Graph\\n5. Testing aller Modelle (Word2Vec, Doc2Vec, LinkPred, Know. Graph)\\n6. Weitere Beispielvorführung\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inhaltverzeichnis:\n",
    "'''\n",
    "1. Datenaufbereitung\n",
    "2. Methodendeklaration, Hilfsfunktion, unsw.\n",
    "3. Training (Word2Vec, Doc2Vec)\n",
    "4. Laden der Training-Ergebnisse von LinkPred, Knowledge Graph\n",
    "5. Testing aller Modelle (Word2Vec, Doc2Vec, LinkPred, Know. Graph)\n",
    "6. Weitere Beispielvorführung\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f1854-0b14-4c93-b1b2-ccef2e425d6c",
   "metadata": {},
   "source": [
    "## *1. Datenaufbereitung*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc8c7a76-ca00-4b83-94f2-01ef7f63b62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001B1804DE680>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/gensim/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001B1804DE980>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/gensim/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001B1804DEB30>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/gensim/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001B1804DECE0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/gensim/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001B1804DEE90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/gensim/\n",
      "ERROR: Could not find a version that satisfies the requirement gensim (from versions: none)\n",
      "ERROR: No matching distribution found for gensim\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --force-reinstall gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27255109-b67f-45e4-8c7c-20aad52e1ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001199EE0E5F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001199EE0E8F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001199EE0EAA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001199EE0EC50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001199EE0EE00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/numpy/\n",
      "ERROR: Could not find a version that satisfies the requirement numpy (from versions: none)\n",
      "ERROR: No matching distribution found for numpy\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --force-reinstall numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "160d14d7-56c7-4e27-84c2-5c8c24f53932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d7a7f37-7baf-4405-afca-5222c96e89d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(column):\n",
    "    return str(column).split(\",\")\n",
    "\n",
    "'''\n",
    "def open_csv_training():\n",
    "    df_data = pd.read_excel(\"CompleteDataClusteredCleaned_TrainingData_4024Entries.xlsx\")\n",
    "    df_data = df_data.drop(columns=['Unnamed: 0', 'JobTitle', 'Description', 'Skills/Description'])\n",
    "    df_data = df_data[['Cluster', 'NewSkills_lowercase']]\n",
    "    return df_data\n",
    "\n",
    "def open_csv_testdata():\n",
    "    df_data = pd.read_excel(\"CompleteDataClusteredCleaned_TestingData_1006Entries.xlsx\")\n",
    "    df_data = df_data.drop(columns=['Unnamed: 0', 'JobTitle', 'Description', 'Skills/Description'])\n",
    "    df_data = df_data[['Cluster', 'NewSkills_lowercase']]\n",
    "    return df_data\n",
    "'''\n",
    "\n",
    "def open_csv_training():\n",
    "    df_data = pd.read_csv(\"Complete_Data_Clustered_Cleaned.csv\")\n",
    "    df_data = df_data.drop(columns=['Unnamed: 0', 'JobTitle', 'Description', 'Skills/Description'])\n",
    "    df_data = df_data[['Cluster', 'NewSkills_lowercase']]\n",
    "    #df_data = df_data[['newCluster', 'NewSkills_lowercase']] # this is for Complete_Data_Clustered_Cleaned_test_v2.csv\n",
    "    return df_data\n",
    "\n",
    "def open_csv_testdata():\n",
    "    df_data = pd.read_csv(\"Complete_Data_Clustered_Cleaned_test.csv\")\n",
    "    df_data = df_data.drop(columns=['Unnamed: 0', 'JobTitle', 'Description', 'Skills/Description'])\n",
    "    df_data = df_data[['Cluster', 'NewSkills_lowercase']]\n",
    "    #df_data = df_data[['newCluster', 'NewSkills_lowercase']] # this is for Complete_Data_Clustered_Cleaned_test_v2.csv\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9bd0c8-3c33-4768-be0f-5331279b26f0",
   "metadata": {},
   "source": [
    "## *2. Methodendeklaration, Hilfsfunktion, unsw.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e07cf8e-70fb-4b6f-b6a3-9fb558841c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "### Training uses the Doc2Vec or Word2Vec model which simply embeds words according to how often they occur together (ignoring order afaik)\n",
    "## Adapt vector size and epochs in training once data set is complete\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import ast\n",
    "import numpy\n",
    "\n",
    "def trainWord2Vec(data):\n",
    "# Preprocess the data and create list of skills (this training does NOT include jobs!)\n",
    "    tagged_data = [ast.literal_eval(skills) for job, skills in data]\n",
    "\n",
    "# Train the Word2Vec model, try different vector sizes for interesting effects in similarities\n",
    "    model = Word2Vec(vector_size=350, min_count=3, workers=6, epochs=150)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model\n",
    "\n",
    "def trainDoc2Vec(data):\n",
    "    tagged_data = [TaggedDocument(words=ast.literal_eval(skills), tags=[job]) for job, skills in data[:]]\n",
    "\n",
    "# Train the Doc2Vec model\n",
    "    model = Doc2Vec(vector_size=20, window=5, min_count=3, workers=6, epochs=20)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model\n",
    "\n",
    "# helper method to calculate average embedding vector of a list of string skills, filters out skills unknown to the model\n",
    "def averageOfSkills(model, input, axis = 0):\n",
    "    vectors = [model.wv[i] for i in input if model.wv.has_index_for(i)]\n",
    "    if(len(vectors) == 0):\n",
    "        return numpy.zeros(len(model.wv[0]))\n",
    "    return numpy.average(vectors, axis = axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5288fabf-b069-4492-996a-8083fcde9b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFine-Tuning Parameters for the Models:\\nVector Size (vector_size): Start with a larger vector size, such as 200 or 300. Larger vectors can capture more complex word relationships.\\n\\nMinimum Count (min_count): You can set this to a value between 1 and 5. A higher value like 3 can help filter out rare words and reduce noise in the embeddings.\\n\\nWorkers (workers): Set this to the number of available CPU cores for faster training. If you have more CPU cores, you can set workers to a higher value (e.g., 4 or 8).\\n\\nEpochs (epochs): Increase the number of epochs to 100 or even higher. More epochs allow the model to see the data more times and potentially learn better embeddings.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Fine-Tuning Parameters for the Models:\n",
    "Vector Size (vector_size): Start with a larger vector size, such as 200 or 300. Larger vectors can capture more complex word relationships.\n",
    "\n",
    "Minimum Count (min_count): You can set this to a value between 1 and 5. A higher value like 3 can help filter out rare words and reduce noise in the embeddings.\n",
    "\n",
    "Workers (workers): Set this to the number of available CPU cores for faster training. If you have more CPU cores, you can set workers to a higher value (e.g., 4 or 8).\n",
    "\n",
    "Epochs (epochs): Increase the number of epochs to 100 or even higher. More epochs allow the model to see the data more times and potentially learn better embeddings.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6acefa7f-0518-4f87-b472-e48543ce780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def calcAvgEmbeddings(model, data):\n",
    "    # Tests for the embedding go here\n",
    "    # print(model.wv.most_similar(\"neural_networks\", topn = 5))\n",
    "\n",
    "    # split the tuples\n",
    "    job_titles, skills = list(zip(*data))\n",
    "    # calculate the average skill vector of every job offering / person and zip back together\n",
    "    skillAverages = [averageOfSkills(model, ast.literal_eval(skill)) for skill in skills]\n",
    "    data_averaged = list(zip(job_titles, skillAverages))\n",
    "\n",
    "    return (skillAverages, data_averaged)\n",
    "\n",
    "# This gives us a list of job offerings and average embeddings. \n",
    "# Could be used as input to a graph neural network or knowledge graph?\n",
    "# Is used below to classify immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "535bf8cc-b2c5-4e58-8850-f51c92ec0ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply suggest smallest distance to user's average (\"Option 2\") using word2Vec\n",
    "import itertools\n",
    "import operator\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def prepareOption2(data):\n",
    "    model = trainWord2Vec(data)\n",
    "    skillAverages, data_averaged = calcAvgEmbeddings(model, data)\n",
    "\n",
    "    # sort before grouping\n",
    "    data_averaged.sort(key=operator.itemgetter(0))\n",
    "    # Group by job title, take the average embedding of everyone with that title and make a dictionary (maps job title to average embedding)\n",
    "    job_averages = {key : numpy.average(list(zip(*list(value)))[1], axis = 0)\n",
    "        for key, value in itertools.groupby(data_averaged, lambda x: x[0])}\n",
    "\n",
    "    #keep keys and values\n",
    "    keys = list(job_averages.keys())\n",
    "    values = list(job_averages.values())\n",
    "    # easy access to the avg vector\n",
    "    #print(job_averages['Advisor, Data Science'])\n",
    "    \n",
    "    # model.save(\"word2vec_model_option2.bin\")\n",
    "    return job_averages, keys, values, model\n",
    "\n",
    "# usage in next cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5390225b-3dbb-440a-91d9-872e5bbce6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Option 3\" could be using the embedding learned by Doc2Vec and doing the same manual averaging as Option 2 for \"learning\" the correlation of job to skills\n",
    "import itertools\n",
    "import operator\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "\n",
    "def prepareOption3(data):\n",
    "#if !data:\n",
    "#  data = open_csv_clean()\n",
    "    model = trainDoc2Vec(data)\n",
    "    skillAverages, data_averaged = calcAvgEmbeddings(model, data)\n",
    "\n",
    "    # sort before grouping\n",
    "    data_averaged.sort(key=operator.itemgetter(0))\n",
    "    # Group by job title, take the average embedding of everyone with that title and make a dictionary (maps job title to average embedding)\n",
    "    job_averages = {key : numpy.average(list(zip(*list(value)))[1], axis = 0)\n",
    "        for key, value in itertools.groupby(data_averaged, lambda x: x[0])}\n",
    "\n",
    "    #keep keys and values\n",
    "    keys = list(job_averages.keys())\n",
    "    values = list(job_averages.values())\n",
    "    \n",
    "    return job_averages, keys, values, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140d19b6-0345-4e41-9fb8-2b08e68d7180",
   "metadata": {},
   "source": [
    "## *3. Training (Word2Vec, Doc2Vec)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c47e2f1-2cf5-4f6f-9e41-6baed42f206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING FOR DOC2VEC, WORD2VEC (Cluster mit 0 bis 51)\n",
    "\n",
    "#Training mit seperaten Excel Dateien\n",
    "import heapq\n",
    "\n",
    "trainingdata = open_csv_training()\n",
    "testingdata = open_csv_testdata()\n",
    "\n",
    "formatted_data = trainingdata.values.tolist()\n",
    "formatted_test = testingdata.values.tolist()\n",
    "jobs, skills = list(zip(*formatted_test))\n",
    "skills = [ast.literal_eval(skill) for skill in skills]\n",
    "\n",
    "# Option 1: Doc2Vec\n",
    "model_option1 = trainDoc2Vec(formatted_data)\n",
    "\n",
    "# Option 2: Word2Vec\n",
    "job_averages_option2, keys_option2, values_option2, model_option2 = prepareOption2(formatted_data)\n",
    "\n",
    "# Option 3: Doc2Vec Embedding\n",
    "job_averages_option3, keys_option3, values_option3, model_option3 = prepareOption3(formatted_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e27650-9364-4c02-821f-7d91e9110456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding ausprinten\n",
    "\n",
    "'''\n",
    "print(\"Option 2 (Word2Vec) Embedding:\")\n",
    "embedding_option2 = model_option2.wv\n",
    "for word, vector in zip(embedding_option2.index_to_key[:20], embedding_option2.vectors[:20]):\n",
    "    print(f\"{word}: {vector}\")\n",
    "print(\"Embedding structure/format:\")\n",
    "print(type(embedding_option2))\n",
    "print(f\"Vocabulary size: {len(embedding_option2)}\")\n",
    "print(f\"Embedding dimension: {embedding_option2.vector_size}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18143b5c-38f1-495b-ae64-3b8a9e2b06f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding ausprinten\n",
    "'''\n",
    "print(\"Option 3 (Doc2Vec) Embedding:\")\n",
    "embedding_option3 = model_option3.dv\n",
    "for tag, vector in zip(embedding_option3.index_to_key[:20], embedding_option3.vectors[:20]):\n",
    "    print(f\"{tag}: {vector}\")\n",
    "print(\"Embedding structure/format:\")\n",
    "print(type(embedding_option3))\n",
    "print(f\"Vocabulary size: {len(embedding_option3)}\")\n",
    "print(f\"Embedding dimension: {embedding_option3.vector_size}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de77a912-42ab-48a7-883f-07f5c565677f",
   "metadata": {},
   "source": [
    "## *4. Laden der Training-Ergebnisse von LinkPred, Knowledge Graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19898036-531c-4e92-924a-6f8822453825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file = 'linkPred_Output.csv'\n",
    "\n",
    "linkPred_Output = []\n",
    "with open(csv_file, 'r', newline='') as file:\n",
    "    reader = csv.reader(file, delimiter=',')\n",
    "    next(reader)  # Skip the header (first row)\n",
    "    for row in reader:\n",
    "        # Extract the second, third, and fourth (last) columns\n",
    "        extracted_row = [int(row[1]), int(row[2]), int(row[3])]\n",
    "        linkPred_Output.append(extracted_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95fec4d-f216-4212-829e-93afeec95b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_csv_testdata_LinkPred():\n",
    "    df_data = pd.read_csv(\"Complete_Data_Clustered_Cleaned_test_v2.csv\")\n",
    "    df_data = df_data.drop(columns=['Unnamed: 0', 'JobTitle', 'Description', 'Skills/Description'])\n",
    "    df_data = df_data[['newCluster', 'NewSkills_lowercase']] # this is for Complete_Data_Clustered_Cleaned_test_v2.csv\n",
    "    return df_data\n",
    "\n",
    "groundlinkpred = open_csv_testdata_LinkPred()\n",
    "groundlinkpred = groundlinkpred[\"newCluster\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f95cd0-180c-4232-8867-8e44be2e2b0f",
   "metadata": {},
   "source": [
    "## *5. Testing aller Modelle (Word2Vec, Doc2Vec, LinkPred, Know. Graph)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8de702-fc3c-4f07-b917-15e120093b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Doc2Vec\n",
    "predictions_option1 = []\n",
    "for skills_example in skills:\n",
    "    infer_vector_option1 = model_option1.infer_vector(skills_example)\n",
    "    similar_jobs_option1 = model_option1.dv.most_similar([infer_vector_option1], topn=3)\n",
    "    predicted_jobs_option1 = [job for job, similarity in similar_jobs_option1]\n",
    "    predictions_option1.append(predicted_jobs_option1)\n",
    "\n",
    "# Option 2: Word2Vec\n",
    "predictions_option2 = []\n",
    "for skills_example in skills:\n",
    "    avg_option2 = averageOfSkills(model_option2, skills_example)\n",
    "    distVec_option2 = cdist([avg_option2], values_option2)\n",
    "    topJobs_option2 = [k for dist, k in heapq.nsmallest(3, zip(distVec_option2.transpose(), keys_option2))]\n",
    "    predictions_option2.append(topJobs_option2)\n",
    "\n",
    "# Option 3: Doc2Vec Embedding\n",
    "predictions_option3 = []\n",
    "for skills_example in skills:\n",
    "    avg_option3 = averageOfSkills(model_option3, skills_example)\n",
    "    distVec_option3 = cdist([avg_option3], values_option3)\n",
    "    topJobs_option3 = [k for dist, k in heapq.nsmallest(3, zip(distVec_option3.transpose(), keys_option3))]\n",
    "    predictions_option3.append(topJobs_option3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2629fda8-9cc4-476c-ab62-662eea8184ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "• True positive (TP): A test result that correctly indicates the presence of a condition or characteristic.\n",
    "• False positive (FP): A test result that wrongly indicates that a particular condition or attribute is present.\n",
    "'''\n",
    "\n",
    "# job[0] is the label/correct job category\n",
    "ground_truth_labels = [job[0] for job in formatted_test]\n",
    "\n",
    "# Option 1\n",
    "tp_option1 = 0\n",
    "fp_option1 = 0\n",
    "\n",
    "# Iterate over predicted job titles and ground truth labels\n",
    "for predicted_jobs, ground_truth in zip(predictions_option1, ground_truth_labels):\n",
    "    if ground_truth in predicted_jobs:\n",
    "        tp_option1 += 1\n",
    "    else:\n",
    "        fp_option1 += 1\n",
    "\n",
    "# Option 2\n",
    "tp_option2 = 0\n",
    "fp_option2 = 0\n",
    "\n",
    "# Iterate over predicted job titles and ground truth labels\n",
    "for predicted_jobs, ground_truth in zip(predictions_option2, ground_truth_labels):\n",
    "    if ground_truth in predicted_jobs:\n",
    "        tp_option2 += 1\n",
    "    else:\n",
    "        fp_option2 += 1\n",
    "\n",
    "# Option 3\n",
    "tp_option3 = 0\n",
    "fp_option3 = 0\n",
    "\n",
    "# Iterate over predicted job titles and ground truth labels\n",
    "for predicted_jobs, ground_truth in zip(predictions_option3, ground_truth_labels):\n",
    "    if ground_truth in predicted_jobs:\n",
    "        tp_option3 += 1\n",
    "    else:\n",
    "        fp_option3 += 1\n",
    "\n",
    "print(\"Option 1:\")\n",
    "print(\"True Positives (TP):\", tp_option1)\n",
    "print(\"False Positives (FP):\", fp_option1)\n",
    "\n",
    "print(\"\\nOption 2:\")\n",
    "print(\"True Positives (TP):\", tp_option2)\n",
    "print(\"False Positives (FP):\", fp_option2)\n",
    "\n",
    "print(\"\\nOption 3:\")\n",
    "print(\"True Positives (TP):\", tp_option3)\n",
    "print(\"False Positives (FP):\", fp_option3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a844ab88-5bdb-49c7-9adf-71aa4122badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link Prediction Sonderfall (weil Cluster nur zwischen 0 und 20 statt 0 und 51)\n",
    "\n",
    "# I NEED A NEW GROUND TRUTH LABE\n",
    "\n",
    "# Option 4: Link Prediction\n",
    "tp_option4 = 0\n",
    "fp_option4 = 0\n",
    "\n",
    "# Iterate over predicted job titles and ground truth labels\n",
    "for predicted_jobs, ground_truth in zip(linkPred_Output, groundlinkpred):\n",
    "    if ground_truth in predicted_jobs:\n",
    "        tp_option4 += 1\n",
    "    else:\n",
    "        fp_option4 += 1\n",
    "        \n",
    "print(\"\\nOption 4:\")\n",
    "print(\"True Positives (TP):\", tp_option4)\n",
    "print(\"False Positives (FP):\", fp_option4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6279851-1668-468b-8317-82ca652bea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "• Accuracy (ACC): The proportion of correct predictions (both true positives and true negatives) out of all predictions made by the model.\n",
    "• Precision or positive predictive value (PPV): The proportion of true positives out of all positive predictions made by the model.\n",
    "'''\n",
    "# Calculate precision (PPV)\n",
    "precision_option1 = tp_option1 / (tp_option1 + fp_option1)\n",
    "precision_option2 = tp_option2 / (tp_option2 + fp_option2)\n",
    "precision_option3 = tp_option3 / (tp_option3 + fp_option3)\n",
    "precision_option4 = tp_option4 / (tp_option4 + fp_option4)\n",
    "\n",
    "# Calculate accuracy (ACC)\n",
    "total_predictions = len(ground_truth_labels)\n",
    "accuracy_option1 = tp_option1 / total_predictions\n",
    "accuracy_option2 = tp_option2 / total_predictions\n",
    "accuracy_option3 = tp_option3 / total_predictions\n",
    "accuracy_option4 = tp_option4 / total_predictions\n",
    "\n",
    "print(\"Option 1:\")\n",
    "print(\"True Positives (TP):\", tp_option1)\n",
    "print(\"False Positives (FP):\", fp_option1)\n",
    "print(\"Precision (PPV):\", precision_option1)\n",
    "print(\"Accuracy (ACC):\", accuracy_option1)\n",
    "\n",
    "print(\"\\nOption 2:\")\n",
    "print(\"True Positives (TP):\", tp_option2)\n",
    "print(\"False Positives (FP):\", fp_option2)\n",
    "print(\"Precision (PPV):\", precision_option2)\n",
    "print(\"Accuracy (ACC):\", accuracy_option2)\n",
    "\n",
    "print(\"\\nOption 3:\")\n",
    "print(\"True Positives (TP):\", tp_option3)\n",
    "print(\"False Positives (FP):\", fp_option3)\n",
    "print(\"Precision (PPV):\", precision_option3)\n",
    "print(\"Accuracy (ACC):\", accuracy_option3)\n",
    "\n",
    "print(\"\\nOption 4:\")\n",
    "print(\"True Positives (TP):\", tp_option4)\n",
    "print(\"False Positives (FP):\", fp_option4)\n",
    "print(\"Precision (PPV):\", precision_option4)\n",
    "print(\"Accuracy (ACC):\", accuracy_option4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331cd246-6806-41ae-beee-10af3686cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Ergebnisse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Accuracy values for the three options\n",
    "accuracies = [accuracy_option1, accuracy_option2, accuracy_option3, accuracy_option4, 54/298]\n",
    "\n",
    "# Labels for the x-axis\n",
    "options = ['Doc2Vec', 'Word2Vec', 'Word/Doc2Vec', 'LinkPred', 'Knowledge Graph']\n",
    "\n",
    "# Plotting the bar graph\n",
    "plt.bar(options, accuracies)\n",
    "#plt.xlabel('Options')\n",
    "plt.ylabel('Testing Accuracy in %')\n",
    "plt.title('Accuracy Comparison of different Models')\n",
    "plt.ylim([0, 1])  # Set the y-axis limits between 0 and 1\n",
    "\n",
    "# Add percentages above the bars\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.text(i, acc + 0.01, f'{acc*100:.2f}%', ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43595206-f507-41a3-bed9-57ca7560dbc5",
   "metadata": {},
   "source": [
    "## *6. Weitere Beispielvorführung*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f158b-a8e2-4f83-8363-b9aa1cc90236",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_example = ['cad', 'maschinenbau', 'kostruktion', 'fräsen', 'drehen', 'montieren']\n",
    "\n",
    "# Option 1: Doc2Vec\n",
    "infer_vector_option1 = model_option1.infer_vector(skills_example)\n",
    "similar_jobs_option1 = model_option1.dv.most_similar([infer_vector_option1], topn=3)\n",
    "predicted_jobs_option1 = [job for job, similarity in similar_jobs_option1]\n",
    "print(\"Option 1 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match:\", predicted_jobs_option1)\n",
    "print(\"\")\n",
    "\n",
    "# Option 2: Word2Vec\n",
    "avg_option2 = averageOfSkills(model_option2, skills_example)\n",
    "distVec_option2 = cdist([avg_option2], values_option2)\n",
    "topJobs_option2 = [k for dist, k in heapq.nsmallest(3, zip(distVec_option2.transpose(), keys_option2))]\n",
    "print(\"Option 2 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match:\", topJobs_option2)\n",
    "print(\"\")\n",
    "\n",
    "# Option 3: Doc2Vec Embedding\n",
    "avg_option3 = averageOfSkills(model_option3, skills_example)\n",
    "distVec_option3 = cdist([avg_option3], values_option3)\n",
    "topJobs_option3 = [k for dist, k in heapq.nsmallest(3, zip(distVec_option3.transpose(), keys_option3))]\n",
    "print(\"Option 3 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match:\", topJobs_option3)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf84f56-ab2c-4b32-a5f9-4f5523d63365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b6b43-98d3-4e7b-a6cf-27bc58e2dedf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
