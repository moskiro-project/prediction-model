{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a74965-7c85-4d75-be00-9068d7a967dd",
   "metadata": {},
   "source": [
    "# Modelle (Word2Vec, Doc2Vec) sowie Evaluation aller Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8e7030c-3a8c-4a74-b549-87489393fda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Datenaufbereitung\\n2. Methodendeklaration, Hilfsfunktion, unsw.\\n3. Training (Word2Vec, Doc2Vec)\\n4. Laden der Training-Ergebnisse von LinkPred, Knowledge Graph\\n5. Testing aller Modelle (Word2Vec, Doc2Vec, LinkPred, Know. Graph)\\n6. Weitere Beispielvorführung\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inhaltverzeichnis:\n",
    "'''\n",
    "1. Datenaufbereitung\n",
    "2. Methodendeklaration, Hilfsfunktion, unsw.\n",
    "3. Training (Word2Vec, Doc2Vec)\n",
    "4. Laden der Training-Ergebnisse von LinkPred, Knowledge Graph\n",
    "5. Testing aller Modelle (Word2Vec, Doc2Vec, LinkPred, Know. Graph)\n",
    "6. Weitere Beispielvorführung\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f1854-0b14-4c93-b1b2-ccef2e425d6c",
   "metadata": {},
   "source": [
    "## *1. Datenaufbereitung*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e95fbab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following versions that require a different python version: 1.0.0rc1 Requires-Python >=3.10,<3.11\n",
      "ERROR: Could not find a version that satisfies the requirement emgraph (from versions: none)\n",
      "ERROR: No matching distribution found for emgraph\n",
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install emgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "160d14d7-56c7-4e27-84c2-5c8c24f53932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from LinkPred import model\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import ast\n",
    "import numpy as np\n",
    "import itertools\n",
    "import operator\n",
    "from scipy.spatial.distance import cdist\n",
    "import heapq\n",
    "import KG_model as graph\n",
    "import importlib\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f9e54da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up some global variables regarding data format\n",
    "training_file = \"data\\Complete_Data_Clustered_Cleaned.csv\"\n",
    "test_file = \"data\\Complete_Data_Clustered_Cleaned_test.csv\"\n",
    "\n",
    "cluster_column = 'Cluster'\n",
    "skill_column = 'NewSkills_lowercase'\n",
    "training_columns = ['Cluster', 'NewSkills_lowercase']\n",
    "test_columns = ['Cluster', 'NewSkills_lowercase']\n",
    "\n",
    "ground_truth_file = 'data/test_data_graph_org_new.csv'\n",
    "\n",
    "totalClusters = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d7a7f37-7baf-4405-afca-5222c96e89d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(column):\n",
    "    return str(column).split(\",\")\n",
    "\n",
    "def open_csv_training():\n",
    "    df_data = pd.read_csv(training_file)\n",
    "    df_data = df_data.drop(columns=['Unnamed: 0', 'JobTitle', 'Description', 'Skills/Description'])\n",
    "    df_data = df_data[training_columns]\n",
    "    return df_data\n",
    "\n",
    "def open_csv_testdata():\n",
    "    df_data = pd.read_csv(test_file)\n",
    "    df_data = df_data.drop(columns=['Unnamed: 0', 'JobTitle', 'Description', 'Skills/Description'])\n",
    "    df_data = df_data[test_columns]\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9bd0c8-3c33-4768-be0f-5331279b26f0",
   "metadata": {},
   "source": [
    "## *2. Methodendeklaration, Hilfsfunktion, unsw.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e07cf8e-70fb-4b6f-b6a3-9fb558841c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training uses the Doc2Vec or Word2Vec model which simply embeds words according to how often they occur together (ignoring order afaik)\n",
    "## Adapt vector size and epochs in training once data set is complete\n",
    "\n",
    "\n",
    "\n",
    "def trainWord2Vec(data):\n",
    "# Preprocess the data and create list of skills (this training does NOT include jobs!)\n",
    "    tagged_data = [ast.literal_eval(skills) for job, skills in data]\n",
    "\n",
    "# Train the Word2Vec model, try different vector sizes for interesting effects in similarities\n",
    "    model = Word2Vec(vector_size=50, min_count=1, workers=4, epochs=20)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model\n",
    "\n",
    "def trainDoc2Vec(data):\n",
    "    tagged_data = [TaggedDocument(words=ast.literal_eval(skills), tags=[job]) for job, skills in data[:]]\n",
    "\n",
    "# Train the Doc2Vec model\n",
    "    model = Doc2Vec(vector_size=100, window=5, min_count=1, workers=4, epochs=20)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model\n",
    "\n",
    "# helper method to calculate average embedding vector of a list of string skills, filters out skills unknown to the model\n",
    "def averageOfSkills(model, input, axis = 0):\n",
    "    vectors = [model.wv[i] for i in input if model.wv.has_index_for(i)]\n",
    "    if(len(vectors) == 0):\n",
    "        return np.zeros(len(model.wv[0]))\n",
    "    return np.average(vectors, axis = axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6acefa7f-0518-4f87-b472-e48543ce780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calcAvgEmbeddings(model, data):\n",
    "    # Tests for the embedding go here\n",
    "    # print(model.wv.most_similar(\"neural_networks\", topn = 5))\n",
    "\n",
    "    # split the tuples\n",
    "    job_titles, skills = list(zip(*data))\n",
    "    # calculate the average skill vector of every job offering / person and zip back together\n",
    "    skillAverages = [averageOfSkills(model, ast.literal_eval(skill)) for skill in skills]\n",
    "    data_averaged = list(zip(job_titles, skillAverages))\n",
    "\n",
    "    return (skillAverages, data_averaged)\n",
    "\n",
    "# This gives us a list of job offerings and average embeddings. \n",
    "# Could be used as input to a graph neural network or knowledge graph?\n",
    "# Is used below to classify immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "535bf8cc-b2c5-4e58-8850-f51c92ec0ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply suggest smallest distance to user's average (\"Option 2\") using word2Vec\n",
    "\n",
    "def prepareOption2(data):\n",
    "    model = trainWord2Vec(data)\n",
    "    skillAverages, data_averaged = calcAvgEmbeddings(model, data)\n",
    "\n",
    "    # sort before grouping\n",
    "    data_averaged.sort(key=operator.itemgetter(0))\n",
    "    # Group by job title, take the average embedding of everyone with that title and make a dictionary (maps job title to average embedding)\n",
    "    job_averages = {key : np.average(list(zip(*list(value)))[1], axis = 0)\n",
    "        for key, value in itertools.groupby(data_averaged, lambda x: x[0])}\n",
    "\n",
    "    #keep keys and values\n",
    "    keys = list(job_averages.keys())\n",
    "    values = list(job_averages.values())\n",
    "    # easy access to the avg vector\n",
    "    #print(job_averages['Advisor, Data Science'])\n",
    "    \n",
    "    # model.save(\"word2vec_model_option2.bin\")\n",
    "    return job_averages, keys, values, model\n",
    "\n",
    "# usage in next cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5390225b-3dbb-440a-91d9-872e5bbce6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Option 3\" could be using the embedding learned by Doc2Vec and doing the same manual averaging as Option 2 for \"learning\" the correlation of job to skills\n",
    "\n",
    "\n",
    "def prepareOption3(data):\n",
    "#if !data:\n",
    "#  data = open_csv_clean()\n",
    "    model = trainDoc2Vec(data)\n",
    "    skillAverages, data_averaged = calcAvgEmbeddings(model, data)\n",
    "\n",
    "    # sort before grouping\n",
    "    data_averaged.sort(key=operator.itemgetter(0))\n",
    "    # Group by job title, take the average embedding of everyone with that title and make a dictionary (maps job title to average embedding)\n",
    "    job_averages = {key : np.average(list(zip(*list(value)))[1], axis = 0)\n",
    "        for key, value in itertools.groupby(data_averaged, lambda x: x[0])}\n",
    "\n",
    "    #keep keys and values\n",
    "    keys = list(job_averages.keys())\n",
    "    values = list(job_averages.values())\n",
    "    \n",
    "    return job_averages, keys, values, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140d19b6-0345-4e41-9fb8-2b08e68d7180",
   "metadata": {},
   "source": [
    "## *3. Training (Word2Vec, Doc2Vec)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50b0c592-5d4d-4537-9f53-dfd6a76f27a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFull Dataset length = 5030\\nTrainingSet length = 4024 (80%)\\nTestSet length = 1006 (20%)\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80% training, 20% testing\n",
    "'''\n",
    "Full Dataset length = 5030\n",
    "TrainingSet length = 4024 (80%)\n",
    "TestSet length = 1006 (20%)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c47e2f1-2cf5-4f6f-9e41-6baed42f206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training mit seperaten Excel Dateien\n",
    "\n",
    "trainingdata = open_csv_training()\n",
    "testingdata = open_csv_testdata()\n",
    "\n",
    "formatted_data = trainingdata.values.tolist()\n",
    "formatted_test = testingdata.values.tolist()\n",
    "jobs, skills = list(zip(*formatted_test))\n",
    "skills = [ast.literal_eval(skill) for skill in skills]\n",
    "\n",
    "# Option 1: Doc2Vec\n",
    "model_option1 = trainDoc2Vec(formatted_data)\n",
    "\n",
    "# Option 2: Word2Vec\n",
    "job_averages_option2, keys_option2, values_option2, model_option2 = prepareOption2(formatted_data)\n",
    "\n",
    "# Option 3: Doc2Vec Embedding\n",
    "job_averages_option3, keys_option3, values_option3, model_option3 = prepareOption3(formatted_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de77a912-42ab-48a7-883f-07f5c565677f",
   "metadata": {},
   "source": [
    "## *4. Laden der Training-Ergebnisse von LinkPred, Knowledge Graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19898036-531c-4e92-924a-6f8822453825",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkpred = model(load_model=True,save_model=False,load_test=True)\n",
    "linkpred_out = linkpred.test(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360bd0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./Emgraph2/emgraph')\n",
    "sys.path.append('./Emgraph2')\n",
    "graph = importlib.reload(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465b4fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this internally also prepares the new CSV format required by the graph model\n",
    "graph_model = graph.model(epochs=1, train_file=training_file, test_file = test_file, totalClusters= totalClusters, ground_truth_file=ground_truth_file)\n",
    "graph_model.train()\n",
    "graph_model_out = graph_model.test(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f95cd0-180c-4232-8867-8e44be2e2b0f",
   "metadata": {},
   "source": [
    "## *5. Testing aller Modelle (Word2Vec, Doc2Vec, LinkPred, Know. Graph)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8de702-fc3c-4f07-b917-15e120093b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Doc2Vec\n",
    "predictions_option1 = []\n",
    "for skills_example in skills:\n",
    "    infer_vector_option1 = model_option1.infer_vector(skills_example)\n",
    "    similar_jobs_option1 = model_option1.dv.most_similar([infer_vector_option1], topn=3)\n",
    "    predicted_jobs_option1 = [job for job, similarity in similar_jobs_option1]\n",
    "    predictions_option1.append(predicted_jobs_option1)\n",
    "\n",
    "# Option 2: Word2Vec\n",
    "predictions_option2 = []\n",
    "for skills_example in skills:\n",
    "    avg_option2 = averageOfSkills(model_option2, skills_example)\n",
    "    distVec_option2 = cdist([avg_option2], values_option2)\n",
    "    topJobs_option2 = [k for dist, k in heapq.nsmallest(3, zip(distVec_option2.transpose(), keys_option2))]\n",
    "    predictions_option2.append(topJobs_option2)\n",
    "\n",
    "# Option 3: Doc2Vec Embedding\n",
    "predictions_option3 = []\n",
    "for skills_example in skills:\n",
    "    avg_option3 = averageOfSkills(model_option3, skills_example)\n",
    "    distVec_option3 = cdist([avg_option3], values_option3)\n",
    "    topJobs_option3 = [k for dist, k in heapq.nsmallest(3, zip(distVec_option3.transpose(), keys_option3))]\n",
    "    predictions_option3.append(topJobs_option3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2629fda8-9cc4-476c-ab62-662eea8184ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "• True positive (TP): A test result that correctly indicates the presence of a condition or characteristic.\n",
    "• False positive (FP): A test result that wrongly indicates that a particular condition or attribute is present.\n",
    "'''\n",
    "\n",
    "# job[0] is the label/correct job category\n",
    "ground_truth_labels = [job[0] for job in formatted_test]\n",
    "\n",
    "# Option 1\n",
    "tp_option1 = 0\n",
    "fp_option1 = 0\n",
    "\n",
    "# Iterate over predicted job titles and ground truth labels\n",
    "for predicted_jobs, ground_truth in zip(predictions_option1, ground_truth_labels):\n",
    "    if ground_truth in predicted_jobs:\n",
    "        print(\"Ground truth \" + str(ground_truth) + \" in: \" + str(predicted_jobs))\n",
    "        tp_option1 += 1\n",
    "    else:\n",
    "        fp_option1 += 1\n",
    "\n",
    "# Option 2\n",
    "tp_option2 = 0\n",
    "fp_option2 = 0\n",
    "\n",
    "# Iterate over predicted job titles and ground truth labels\n",
    "for predicted_jobs, ground_truth in zip(predictions_option2, ground_truth_labels):\n",
    "    if ground_truth in predicted_jobs:\n",
    "        tp_option2 += 1\n",
    "    else:\n",
    "        fp_option2 += 1\n",
    "\n",
    "# Option 3\n",
    "tp_option3 = 0\n",
    "fp_option3 = 0\n",
    "\n",
    "# Iterate over predicted job titles and ground truth labels\n",
    "for predicted_jobs, ground_truth in zip(predictions_option3, ground_truth_labels):\n",
    "    if ground_truth in predicted_jobs:\n",
    "        tp_option3 += 1\n",
    "    else:\n",
    "        fp_option3 += 1\n",
    "\n",
    "# Option 4: Knowledge Graph\n",
    "tp_option4 = 0\n",
    "fp_option4 = 0\n",
    "df2 = pd.read_csv(ground_truth_file)[\"tail\"].values\n",
    "preds = pd.DataFrame(graph_model_out, columns=[\"Person\", \"Top3 Predictions\"])\n",
    "preds = preds[\"Top3 Predictions\"]\n",
    "for i in range(len(preds)):\n",
    "    if df2[i] in preds[i]:\n",
    "        tp_option4 += 1\n",
    "    else:\n",
    "        fp_option4 += 1\n",
    "\n",
    "print(\"Option 1:\")\n",
    "print(\"True Positives (TP):\", tp_option1)\n",
    "print(\"False Positives (FP):\", fp_option1)\n",
    "\n",
    "print(\"\\nOption 2:\")\n",
    "print(\"True Positives (TP):\", tp_option2)\n",
    "print(\"False Positives (FP):\", fp_option2)\n",
    "\n",
    "print(\"\\nOption 3:\")\n",
    "print(\"True Positives (TP):\", tp_option3)\n",
    "print(\"False Positives (FP):\", fp_option3)\n",
    "\n",
    "print(\"\\nOption 4:\")\n",
    "print(\"True Positives (TP):\", tp_option4)\n",
    "print(\"False Positives (FP):\", fp_option4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6279851-1668-468b-8317-82ca652bea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "• Accuracy (ACC): The proportion of correct predictions (both true positives and true negatives) out of all predictions made by the model.\n",
    "• Precision or positive predictive value (PPV): The proportion of true positives out of all positive predictions made by the model.\n",
    "'''\n",
    "# Calculate precision (PPV)\n",
    "precision_option1 = tp_option1 / (tp_option1 + fp_option1)\n",
    "precision_option2 = tp_option2 / (tp_option2 + fp_option2)\n",
    "precision_option3 = tp_option3 / (tp_option3 + fp_option3)\n",
    "\n",
    "# Calculate accuracy (ACC)\n",
    "total_predictions = len(ground_truth_labels)\n",
    "correct_predictions_option1 = tp_option1\n",
    "correct_predictions_option2 = tp_option2\n",
    "correct_predictions_option3 = tp_option3\n",
    "accuracy_option1 = correct_predictions_option1 / total_predictions\n",
    "accuracy_option2 = correct_predictions_option2 / total_predictions\n",
    "accuracy_option3 = correct_predictions_option3 / total_predictions\n",
    "\n",
    "print(\"Option 1:\")\n",
    "print(\"True Positives (TP):\", tp_option1)\n",
    "print(\"False Positives (FP):\", fp_option1)\n",
    "print(\"Precision (PPV):\", precision_option1)\n",
    "print(\"Accuracy (ACC):\", accuracy_option1)\n",
    "\n",
    "print(\"\\nOption 2:\")\n",
    "print(\"True Positives (TP):\", tp_option2)\n",
    "print(\"False Positives (FP):\", fp_option2)\n",
    "print(\"Precision (PPV):\", precision_option2)\n",
    "print(\"Accuracy (ACC):\", accuracy_option2)\n",
    "\n",
    "print(\"\\nOption 3:\")\n",
    "print(\"True Positives (TP):\", tp_option3)\n",
    "print(\"False Positives (FP):\", fp_option3)\n",
    "print(\"Precision (PPV):\", precision_option3)\n",
    "print(\"Accuracy (ACC):\", accuracy_option3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0430aae-957b-4877-9b66-2f2f9cb16791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisierung der Ergebnisse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Accuracy values for the three options\n",
    "accuracies = [accuracy_option1, accuracy_option2, accuracy_option3]\n",
    "\n",
    "# Labels for the x-axis\n",
    "options = ['Option 1', 'Option 2', 'Option 3']\n",
    "\n",
    "# Plotting the bar graph\n",
    "plt.bar(options, accuracies)\n",
    "#plt.xlabel('Options')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.ylim([0, 1])  # Set the y-axis limits between 0 and 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43595206-f507-41a3-bed9-57ca7560dbc5",
   "metadata": {},
   "source": [
    "## *6. Weitere Beispielvorführung*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d2f158b-a8e2-4f83-8363-b9aa1cc90236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 1 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match: [5, 1, 25]\n",
      "\n",
      "Option 2 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match: [10, 31, 7]\n",
      "\n",
      "Option 3 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match: [10, 7, 12]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "skills_example = ['cad', 'maschinenbau', 'kostruktion', 'fräsen', 'drehen', 'montieren']\n",
    "\n",
    "# Option 1: Doc2Vec\n",
    "infer_vector_option1 = model_option1.infer_vector(skills_example)\n",
    "similar_jobs_option1 = model_option1.dv.most_similar([infer_vector_option1], topn=3)\n",
    "predicted_jobs_option1 = [job for job, similarity in similar_jobs_option1]\n",
    "print(\"Option 1 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match:\", predicted_jobs_option1)\n",
    "print(\"\")\n",
    "\n",
    "# Option 2: Word2Vec\n",
    "avg_option2 = averageOfSkills(model_option2, skills_example)\n",
    "distVec_option2 = cdist([avg_option2], values_option2)\n",
    "topJobs_option2 = [k for dist, k in heapq.nsmallest(3, zip(distVec_option2.transpose(), keys_option2))]\n",
    "print(\"Option 2 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match:\", topJobs_option2)\n",
    "print(\"\")\n",
    "\n",
    "# Option 3: Doc2Vec Embedding\n",
    "avg_option3 = averageOfSkills(model_option3, skills_example)\n",
    "distVec_option3 = cdist([avg_option3], values_option3)\n",
    "topJobs_option3 = [k for dist, k in heapq.nsmallest(3, zip(distVec_option3.transpose(), keys_option3))]\n",
    "print(\"Option 3 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match:\", topJobs_option3)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6a2853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}