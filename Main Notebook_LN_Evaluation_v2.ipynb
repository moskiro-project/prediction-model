{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f096d7-10e2-46cf-8585-f26f401ba090",
   "metadata": {
    "id": "74f096d7-10e2-46cf-8585-f26f401ba090"
   },
   "source": [
    "# 1. Datenaufbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bddd8232-3bb0-4279-aa58-8c99b3d1b977",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 695
    },
    "id": "bddd8232-3bb0-4279-aa58-8c99b3d1b977",
    "outputId": "1b2f930e-9510-4ef4-8f74-95b074cca101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-4.3.1-cp39-cp39-win_amd64.whl (24.0 MB)\n",
      "Collecting numpy>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.24.3-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Collecting scipy>=1.7.0 (from gensim)\n",
      "  Using cached scipy-1.10.1-cp39-cp39-win_amd64.whl (42.5 MB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Installing collected packages: smart-open, numpy, scipy, gensim\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 6.3.0\n",
      "    Uninstalling smart-open-6.3.0:\n",
      "      Successfully uninstalled smart-open-6.3.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.1\n",
      "    Uninstalling scipy-1.10.1:\n",
      "      Successfully uninstalled scipy-1.10.1\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.3.1\n",
      "    Uninstalling gensim-4.3.1:\n",
      "      Successfully uninstalled gensim-4.3.1\n",
      "Successfully installed gensim-4.3.1 numpy-1.24.3 scipy-1.10.1 smart-open-6.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --force-reinstall gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40098027-2663-43a4-965d-93fba60db100",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40098027-2663-43a4-965d-93fba60db100",
    "outputId": "beb878f0-3f10-4631-82d4-bb003dc57a18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-1.24.3-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "Successfully installed numpy-1.24.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --force-reinstall numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vVLXXj1lxA-B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "vVLXXj1lxA-B",
    "outputId": "6bc4e0c5-02cd-49bd-e81f-91b2a96914a8"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files \n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "HgjNb_t9xbLs",
   "metadata": {
    "id": "HgjNb_t9xbLs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f46d7a0-4fe5-40ed-9148-f11620646145",
   "metadata": {
    "id": "2f46d7a0-4fe5-40ed-9148-f11620646145"
   },
   "outputs": [],
   "source": [
    "def helper(column):\n",
    "    return str(column).split(\",\")\n",
    "\n",
    "# Das kann später raus\n",
    "def open_csv_default():\n",
    "    df_data = pd.read_csv(\"naukri_data_science_jobs_india.csv\")\n",
    "    df_data = df_data.drop(columns=['Company', 'Location', 'Job Experience'])\n",
    "    df_data[\"Skills/Description\"] = df_data[\"Skills/Description\"].apply(lambda x: helper(x))\n",
    "    return df_data\n",
    "\n",
    "# Das kann später raus\n",
    "def open_csv_clean():\n",
    "    df_data = pd.read_csv(\"naukri_data_science_jobs_india_cleaned.csv\")\n",
    "    formatted = df_data.values.tolist()\n",
    "    return formatted\n",
    "\n",
    "def open_csv_clustered():\n",
    "    df_data = pd.read_csv(\"naukri_data_science_jobs_india_cleaned_clusterd.csv\")\n",
    "    df_data = df_data.drop(columns=['Job_Role', 'lda_score'])\n",
    "    df_data = df_data[['lda_topic', 'Skills/Description']]\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e8718-046f-4085-9203-91eaa329054d",
   "metadata": {},
   "source": [
    "# 2. Methodendeklaration, Hilfsfunktionen unsw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4z3acAgC4Qf",
   "metadata": {
    "id": "d4z3acAgC4Qf"
   },
   "outputs": [],
   "source": [
    "### Training uses the Doc2Vec or Word2Vec model which simply embeds words according to how often they occur together (ignoring order afaik)\n",
    "## Adapt vector size and epochs in training once data set is complete\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import ast\n",
    "import numpy\n",
    "\n",
    "def trainWord2Vec(data):\n",
    "# Preprocess the data and create list of skills (this training does NOT include jobs!)\n",
    "    tagged_data = [ast.literal_eval(skills) for job, skills in data]\n",
    "\n",
    "# Train the Word2Vec model, try different vector sizes for interesting effects in similarities\n",
    "    model = Word2Vec(vector_size=50, min_count=1, workers=4, epochs=20)\n",
    "    model.build_vocab(tagged_data)\n",
    "\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def trainDoc2Vec(data):\n",
    "    tagged_data = [TaggedDocument(words=ast.literal_eval(skills), tags=[job]) for job, skills in data[:]]\n",
    "\n",
    "# Train the Doc2Vec model\n",
    "    model = Doc2Vec(vector_size=100, window=5, min_count=1, workers=4, epochs=20)\n",
    "    model.build_vocab(tagged_data)\n",
    "\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "    return model\n",
    "\n",
    "# helper method to calculate average embedding vector of a list of string skills, filters out skills unknown to the model\n",
    "def averageOfSkills(model, input, axis = 0):\n",
    "    vectors = [model.wv[i] for i in input if model.wv.has_index_for(i)]\n",
    "    if(len(vectors) == 0):\n",
    "        return numpy.zeros(len(model.wv[0]))\n",
    "    \n",
    "    return numpy.average(vectors, axis = axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eRXCk52dDg83",
   "metadata": {
    "id": "eRXCk52dDg83"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def calcAvgEmbeddings(model, data):\n",
    "\n",
    "    # Tests for the embedding go here\n",
    "    print(model.wv.most_similar(\"neural_networks\", topn = 5))\n",
    "\n",
    "    #cosine distance\n",
    "    print(model.wv.distance(\"finance\", \"machine_learning\"))\n",
    "    print(model.wv.distance(\"ai\", \"machine_learning\"))\n",
    "    print(model.wv.distance(\"business_intelligence\", \"management\"))\n",
    "    print(model.wv.distance('corporate_governance', 'ai'))\n",
    "    #euclidean distance\n",
    "    #print(numpy.linalg.norm(model.wv[\"statistics\"] - model.wv[\"machine_learning\"]))\n",
    "\n",
    "    # split the tuples\n",
    "    job_titles, skills = list(zip(*data))\n",
    "    # calculate the average skill vector of every job offering / person and zip back together\n",
    "    skillAverages = [averageOfSkills(model, ast.literal_eval(skill)) for skill in skills]\n",
    "    data_averaged = list(zip(job_titles, skillAverages))\n",
    "\n",
    "    return (skillAverages, data_averaged)\n",
    "\n",
    "# This gives us a list of job offerings and average embeddings. \n",
    "# Could be used as input to a graph neural network or knowledge graph?\n",
    "# Is used below to classify immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6MOCIrZkKbDX",
   "metadata": {
    "id": "6MOCIrZkKbDX"
   },
   "outputs": [],
   "source": [
    "# simply suggest smallest distance to user's average (\"Option 2\") using word2Vec\n",
    "import itertools\n",
    "import operator\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def prepareOption2(data):\n",
    "\n",
    "    model = trainWord2Vec(data)\n",
    "    skillAverages, data_averaged = calcAvgEmbeddings(model, data)\n",
    "\n",
    "    # sort before grouping\n",
    "    data_averaged.sort(key=operator.itemgetter(0))\n",
    "    # Group by job title, take the average embedding of everyone with that title and make a dictionary (maps job title to average embedding)\n",
    "    job_averages = {key : numpy.average(list(zip(*list(value)))[1], axis = 0)\n",
    "        for key, value in itertools.groupby(data_averaged, lambda x: x[0])}\n",
    "\n",
    "    #keep keys and values\n",
    "    keys = list(job_averages.keys())\n",
    "    values = list(job_averages.values())\n",
    "    # easy access to the avg vector\n",
    "    #print(job_averages['Advisor, Data Science'])\n",
    "    \n",
    "    # model.save(\"word2vec_model_option2.bin\")\n",
    "\n",
    "    return job_averages, keys, values, model\n",
    "\n",
    "# usage in next cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eYYHhscDE1vV",
   "metadata": {
    "id": "eYYHhscDE1vV"
   },
   "outputs": [],
   "source": [
    "# \"Option 3\" could be using the embedding learned by Doc2Vec and doing the same manual averaging as Option 2 for \"learning\" the correlation of job to skills\n",
    "import itertools\n",
    "import operator\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "\n",
    "def prepareOption3(data):\n",
    "#if !data:\n",
    "#  data = open_csv_clean()\n",
    "    model = trainDoc2Vec(data)\n",
    "    skillAverages, data_averaged = calcAvgEmbeddings(model, data)\n",
    "\n",
    "    # sort before grouping\n",
    "    data_averaged.sort(key=operator.itemgetter(0))\n",
    "    # Group by job title, take the average embedding of everyone with that title and make a dictionary (maps job title to average embedding)\n",
    "    job_averages = {key : numpy.average(list(zip(*list(value)))[1], axis = 0)\n",
    "        for key, value in itertools.groupby(data_averaged, lambda x: x[0])}\n",
    "\n",
    "    #keep keys and values\n",
    "    keys = list(job_averages.keys())\n",
    "    values = list(job_averages.values())\n",
    "    \n",
    "    return job_averages, keys, values, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ddfa1f-44a3-48f3-b710-455134a4b304",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WW1p_fgqFIws",
    "outputId": "7b273ef3-9ecc-4400-d7bb-5ce1eb1a33da"
   },
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a5a5060-76e8-4cc2-8513-49dc2d734bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('text_mining', 0.9840205907821655), ('computer_vision', 0.9653641581535339), ('logistic_regression', 0.9486359357833862), ('image_processing', 0.9478869438171387), ('deep_learning', 0.9476275444030762)]\n",
      "0.7342003881931305\n",
      "0.41722267866134644\n",
      "0.4114232063293457\n",
      "0.5586076080799103\n",
      "[('computer_vision', 0.9629616737365723), ('advanced_analytics', 0.961156964302063), ('big_data_analytics', 0.9560986161231995), ('risk_analytics', 0.9463423490524292), ('construction', 0.9426249265670776)]\n",
      "0.5141088664531708\n",
      "0.38097262382507324\n",
      "0.3912978768348694\n",
      "0.6066699028015137\n"
     ]
    }
   ],
   "source": [
    "# Proper testing\n",
    "import heapq\n",
    "\n",
    "# divide data into 75 train, 25 test\n",
    "def train_test_split(df, frac=0.25):\n",
    "    # get random sample \n",
    "    test = df.sample(frac=frac, axis=0)\n",
    "    # get everything but the test sample\n",
    "    train = df.drop(index=test.index)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "df = open_csv_clustered()\n",
    "df_data, df_test = train_test_split(df)\n",
    "formatted_data = df_data.values.tolist()\n",
    "formatted_test = df_test.values.tolist()\n",
    "jobs, skills = list(zip(*formatted_test))\n",
    "skills = [ast.literal_eval(skill) for skill in skills]\n",
    "\n",
    "# Option 1: Doc2Vec\n",
    "model_option1 = trainDoc2Vec(formatted_data)\n",
    "\n",
    "# Option 2: Word2Vec\n",
    "job_averages_option2, keys_option2, values_option2, model_option2 = prepareOption2(formatted_data)\n",
    "\n",
    "# Option 3: Doc2Vec Embedding\n",
    "job_averages_option3, keys_option3, values_option3, model_option3 = prepareOption3(formatted_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97855bb4-8360-4f8e-b76e-b574d00fca56",
   "metadata": {},
   "source": [
    "# 4. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "51b87583-6d5e-4979-97c7-5c4bd8006ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Doc2Vec\n",
    "predictions_option1 = []\n",
    "for skills_example in skills:\n",
    "    infer_vector_option1 = model_option1.infer_vector(skills_example)\n",
    "    similar_jobs_option1 = model_option1.dv.most_similar([infer_vector_option1], topn=3)\n",
    "    predicted_jobs_option1 = [job for job, similarity in similar_jobs_option1]\n",
    "    predictions_option1.append(predicted_jobs_option1)\n",
    "\n",
    "# Option 2: Word2Vec\n",
    "predictions_option2 = []\n",
    "for skills_example in skills:\n",
    "    avg_option2 = averageOfSkills(model_option2, skills_example)\n",
    "    distVec_option2 = cdist([avg_option2], values_option2)\n",
    "    topJobs_option2 = [k for dist, k in heapq.nsmallest(3, zip(distVec_option2.transpose(), keys_option2))]\n",
    "    predictions_option2.append(topJobs_option2)\n",
    "\n",
    "# Option 3: Doc2Vec Embedding\n",
    "predictions_option3 = []\n",
    "for skills_example in skills:\n",
    "    avg_option3 = averageOfSkills(model_option3, skills_example)\n",
    "    distVec_option3 = cdist([avg_option3], values_option3)\n",
    "    topJobs_option3 = [k for dist, k in heapq.nsmallest(3, zip(distVec_option3.transpose(), keys_option3))]\n",
    "    predictions_option3.append(topJobs_option3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8f59f316-ab2e-447c-ab8c-24d01bb1ed98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 1:\n",
      "True Positives (TP): 2023\n",
      "False Positives (FP): 743\n",
      "\n",
      "Option 2:\n",
      "True Positives (TP): 2477\n",
      "False Positives (FP): 289\n",
      "\n",
      "Option 3:\n",
      "True Positives (TP): 2445\n",
      "False Positives (FP): 321\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• True positive (TP): A test result that correctly indicates the presence of a condition or characteristic.\n",
    "• False positive (FP): A test result that wrongly indicates that a particular condition or attribute is present.\n",
    "'''\n",
    "\n",
    "# job[0] is the label/correct job category\n",
    "ground_truth_labels = [job[0] for job in formatted_test]\n",
    "\n",
    "# Option 1\n",
    "tp_option1 = 0\n",
    "fp_option1 = 0\n",
    "\n",
    "# Iterate over predicted job titles and ground truth labels\n",
    "for predicted_jobs, ground_truth in zip(predictions_option1, ground_truth_labels):\n",
    "    if ground_truth in predicted_jobs:\n",
    "        tp_option1 += 1\n",
    "    else:\n",
    "        fp_option1 += 1\n",
    "\n",
    "# Option 2\n",
    "tp_option2 = 0\n",
    "fp_option2 = 0\n",
    "\n",
    "# Iterate over predicted job titles and ground truth labels\n",
    "for predicted_jobs, ground_truth in zip(predictions_option2, ground_truth_labels):\n",
    "    if ground_truth in predicted_jobs:\n",
    "        tp_option2 += 1\n",
    "    else:\n",
    "        fp_option2 += 1\n",
    "\n",
    "# Option 3\n",
    "tp_option3 = 0\n",
    "fp_option3 = 0\n",
    "\n",
    "# Iterate over predicted job titles and ground truth labels\n",
    "for predicted_jobs, ground_truth in zip(predictions_option3, ground_truth_labels):\n",
    "    if ground_truth in predicted_jobs:\n",
    "        tp_option3 += 1\n",
    "    else:\n",
    "        fp_option3 += 1\n",
    "\n",
    "print(\"Option 1:\")\n",
    "print(\"True Positives (TP):\", tp_option1)\n",
    "print(\"False Positives (FP):\", fp_option1)\n",
    "\n",
    "print(\"\\nOption 2:\")\n",
    "print(\"True Positives (TP):\", tp_option2)\n",
    "print(\"False Positives (FP):\", fp_option2)\n",
    "\n",
    "print(\"\\nOption 3:\")\n",
    "print(\"True Positives (TP):\", tp_option3)\n",
    "print(\"False Positives (FP):\", fp_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "25aa4080-b775-47a3-87be-a50c5ec30600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 1:\n",
      "True Positives (TP): 2023\n",
      "False Positives (FP): 743\n",
      "Precision (PPV): 0.7313810556760665\n",
      "Accuracy (ACC): 0.7313810556760665\n",
      "\n",
      "Option 2:\n",
      "True Positives (TP): 2477\n",
      "False Positives (FP): 289\n",
      "Precision (PPV): 0.8955169920462762\n",
      "Accuracy (ACC): 0.8955169920462762\n",
      "\n",
      "Option 3:\n",
      "True Positives (TP): 2445\n",
      "False Positives (FP): 321\n",
      "Precision (PPV): 0.8839479392624728\n",
      "Accuracy (ACC): 0.8839479392624728\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• Accuracy (ACC): The proportion of correct predictions (both true positives and true negatives) out of all predictions made by the model.\n",
    "• Precision or positive predictive value (PPV): The proportion of true positives out of all positive predictions made by the model.\n",
    "'''\n",
    "# Calculate precision (PPV)\n",
    "precision_option1 = tp_option1 / (tp_option1 + fp_option1)\n",
    "precision_option2 = tp_option2 / (tp_option2 + fp_option2)\n",
    "precision_option3 = tp_option3 / (tp_option3 + fp_option3)\n",
    "\n",
    "# Calculate accuracy (ACC)\n",
    "total_predictions = len(ground_truth_labels)\n",
    "correct_predictions_option1 = tp_option1\n",
    "correct_predictions_option2 = tp_option2\n",
    "correct_predictions_option3 = tp_option3\n",
    "accuracy_option1 = correct_predictions_option1 / total_predictions\n",
    "accuracy_option2 = correct_predictions_option2 / total_predictions\n",
    "accuracy_option3 = correct_predictions_option3 / total_predictions\n",
    "\n",
    "print(\"Option 1:\")\n",
    "print(\"True Positives (TP):\", tp_option1)\n",
    "print(\"False Positives (FP):\", fp_option1)\n",
    "print(\"Precision (PPV):\", precision_option1)\n",
    "print(\"Accuracy (ACC):\", accuracy_option1)\n",
    "\n",
    "print(\"\\nOption 2:\")\n",
    "print(\"True Positives (TP):\", tp_option2)\n",
    "print(\"False Positives (FP):\", fp_option2)\n",
    "print(\"Precision (PPV):\", precision_option2)\n",
    "print(\"Accuracy (ACC):\", accuracy_option2)\n",
    "\n",
    "print(\"\\nOption 3:\")\n",
    "print(\"True Positives (TP):\", tp_option3)\n",
    "print(\"False Positives (FP):\", fp_option3)\n",
    "print(\"Precision (PPV):\", precision_option3)\n",
    "print(\"Accuracy (ACC):\", accuracy_option3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cff3be5d-1c14-4c0f-a1ad-86691eb76e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualisierung der Ergebnisse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Accuracy values for the three options\n",
    "accuracies = [accuracy_option1, accuracy_option2, accuracy_option3]\n",
    "\n",
    "# Labels for the x-axis\n",
    "options = ['Option 1', 'Option 2', 'Option 3']\n",
    "\n",
    "# Plotting the bar graph\n",
    "plt.bar(options, accuracies)\n",
    "#plt.xlabel('Options')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.ylim([0, 1])  # Set the y-axis limits between 0 and 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ed430d81-27ce-4cf3-a1b9-936eccedd1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(predictions_option1), len(predictions_option2), len(predictions_option3))\n",
    "# print(len(formatted_test), len(formatted_data), len(df))\n",
    "# print(predictions_option1)\n",
    "\n",
    "# ground_truth_labels = [job[0] for job in formatted_test]\n",
    "# len(ground_truth_labels)\n",
    "# print(ground_truth_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13f09ce-bce0-4dd5-b65d-49fb78b5b768",
   "metadata": {},
   "source": [
    "# 5. Ein Beispiel für Option 1, 2 und 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e5db242-b1e1-477b-8614-9fae142ecc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 1 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match: [1, 7, 6]\n",
      "\n",
      "Option 2 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match: [0, 7, 1]\n",
      "\n",
      "Option 3 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match: [7, 0, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "skills_example = ['azure', 'azure', 'streaming', 'big_data']\n",
    "\n",
    "# Option 1: Doc2Vec\n",
    "infer_vector_option1 = model_option1.infer_vector(skills_example)\n",
    "similar_jobs_option1 = model_option1.dv.most_similar([infer_vector_option1], topn=3)\n",
    "predicted_jobs_option1 = [job for job, similarity in similar_jobs_option1]\n",
    "print(\"Option 1 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match:\", predicted_jobs_option1)\n",
    "print(\"\")\n",
    "\n",
    "# Option 2: Word2Vec\n",
    "avg_option2 = averageOfSkills(model_option2, skills_example)\n",
    "distVec_option2 = cdist([avg_option2], values_option2)\n",
    "topJobs_option2 = [k for dist, k in heapq.nsmallest(3, zip(distVec_option2.transpose(), keys_option2))]\n",
    "print(\"Option 2 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match:\", topJobs_option2)\n",
    "print(\"\")\n",
    "\n",
    "# Option 3: Doc2Vec Embedding\n",
    "avg_option3 = averageOfSkills(model_option3, skills_example)\n",
    "distVec_option3 = cdist([avg_option3], values_option3)\n",
    "topJobs_option3 = [k for dist, k in heapq.nsmallest(3, zip(distVec_option3.transpose(), keys_option3))]\n",
    "print(\"Option 3 Prediction: Given the skillset, it should belong to one of the following three groups beginning with the best match:\", topJobs_option3)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8064fd-8134-48ef-9e4e-3087907e3987",
   "metadata": {},
   "source": [
    "# 6. Evaluierung mit weiterer Kennzahlen / Optional und nicht Fertig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "LA8npAnMSkW6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LA8npAnMSkW6",
    "outputId": "ed793db2-71c2-4631-d83c-eb1b0cbed214"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn the following, I will try to calculate:\\n•\\tCondition positive (P): The number of real positive cases in the data.\\n•\\tCondition negative (N): The number of real negative cases in the data.\\n•\\tTrue positive (TP): A test result that correctly indicates the presence of a condition or characteristic.\\n•\\tTrue negative (TN): A test result that correctly indicates the absence of a condition or characteristic.\\n•\\tFalse positive (FP): A test result that wrongly indicates that a particular condition or attribute is present.\\n•\\tFalse negative (FN): A test result that wrongly indicates that a particular condition or attribute is absent.\\n•\\tSensitivity, recall, hit rate, or true positive rate (TPR): The proportion of true positives out of all actual positive cases.\\n•\\tSpecificity, selectivity, or true negative rate (TNR): The proportion of true negatives out of all actual negative cases.\\n•\\tPrecision or positive predictive value (PPV): The proportion of true positives out of all positive predictions made by the model.\\n•\\tNegative predictive value (NPV): The proportion of true negatives out of all negative predictions made by the model.\\n•\\tMiss rate or false negative rate (FNR): The proportion of false negatives out of all actual positive cases.\\n•\\tFall-out or false positive rate (FPR): The proportion of false positives out of all actual negative cases.\\n•\\tFalse discovery rate (FDR): The proportion of false positives out of all positive predictions made by the model.\\n•\\tFalse omission rate (FOR): The proportion of false negatives out of all negative predictions made by the model.\\n•\\tPositive likelihood ratio (LR+): The ratio of true positive rate to false positive rate.\\n•\\tNegative likelihood ratio (LR-): The ratio of false negative rate to true negative rate.\\n•\\tPrevalence threshold (PT): A threshold value that balances sensitivity and specificity.\\n•\\tThreat score (TS) or critical success index (CSI): The proportion of true positives out of all positive and negative predictions made by the model.\\n•\\tPrevalence: The proportion of positive cases in the data.\\n•\\tAccuracy (ACC): The proportion of correct predictions (both true positives and true negatives) out of all predictions made by the model.\\n•\\tBalanced accuracy (BA): The average of sensitivity and specificity.\\n•\\tF1 score: The harmonic mean of precision and sensitivity, providing a single metric to balance both.\\n•\\tPhi coefficient (φ) or Matthews correlation coefficient (MCC): A correlation coefficient that takes into account true positives, true negatives, false positives, and false negatives.\\n•\\tFowlkes-Mallows index (FM): A geometric mean of precision and sensitivity.\\n•\\tInformedness or bookmaker informedness (BM): The sum of true positive rate and true negative rate, minus 1.\\n•\\tMarkedness (MK) or deltaP (Δp): The sum of positive predictive value and negative predictive value, minus 1.\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation and assessment of binary classification models\n",
    "\n",
    "'''\n",
    "In the following, I will try to calculate:\n",
    "•\tCondition positive (P): The number of real positive cases in the data.\n",
    "•\tCondition negative (N): The number of real negative cases in the data.\n",
    "•\tTrue positive (TP): A test result that correctly indicates the presence of a condition or characteristic.\n",
    "•\tTrue negative (TN): A test result that correctly indicates the absence of a condition or characteristic.\n",
    "•\tFalse positive (FP): A test result that wrongly indicates that a particular condition or attribute is present.\n",
    "•\tFalse negative (FN): A test result that wrongly indicates that a particular condition or attribute is absent.\n",
    "•\tSensitivity, recall, hit rate, or true positive rate (TPR): The proportion of true positives out of all actual positive cases.\n",
    "•\tSpecificity, selectivity, or true negative rate (TNR): The proportion of true negatives out of all actual negative cases.\n",
    "•\tPrecision or positive predictive value (PPV): The proportion of true positives out of all positive predictions made by the model.\n",
    "•\tNegative predictive value (NPV): The proportion of true negatives out of all negative predictions made by the model.\n",
    "•\tMiss rate or false negative rate (FNR): The proportion of false negatives out of all actual positive cases.\n",
    "•\tFall-out or false positive rate (FPR): The proportion of false positives out of all actual negative cases.\n",
    "•\tFalse discovery rate (FDR): The proportion of false positives out of all positive predictions made by the model.\n",
    "•\tFalse omission rate (FOR): The proportion of false negatives out of all negative predictions made by the model.\n",
    "•\tPositive likelihood ratio (LR+): The ratio of true positive rate to false positive rate.\n",
    "•\tNegative likelihood ratio (LR-): The ratio of false negative rate to true negative rate.\n",
    "•\tPrevalence threshold (PT): A threshold value that balances sensitivity and specificity.\n",
    "•\tThreat score (TS) or critical success index (CSI): The proportion of true positives out of all positive and negative predictions made by the model.\n",
    "•\tPrevalence: The proportion of positive cases in the data.\n",
    "•\tAccuracy (ACC): The proportion of correct predictions (both true positives and true negatives) out of all predictions made by the model.\n",
    "•\tBalanced accuracy (BA): The average of sensitivity and specificity.\n",
    "•\tF1 score: The harmonic mean of precision and sensitivity, providing a single metric to balance both.\n",
    "•\tPhi coefficient (φ) or Matthews correlation coefficient (MCC): A correlation coefficient that takes into account true positives, true negatives, false positives, and false negatives.\n",
    "•\tFowlkes-Mallows index (FM): A geometric mean of precision and sensitivity.\n",
    "•\tInformedness or bookmaker informedness (BM): The sum of true positive rate and true negative rate, minus 1.\n",
    "•\tMarkedness (MK) or deltaP (Δp): The sum of positive predictive value and negative predictive value, minus 1.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd76b817-4b0e-45d3-a9e1-8fc022ab10bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a696291-2b34-44ce-919a-3bb1b3606c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground_truth_labels = [job[0] for job in formatted_test]\n",
    "# print(ground_truth_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9666a9a0-7380-4785-8946-f1229c5741a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_jobs_option1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a6f618ac-7f3a-4867-8b64-c2faaea40c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport csv\\n\\ndef save_predictions_to_csv(predictions, file_path):\\n    with open(file_path, \\'w\\', newline=\\'\\') as csvfile:\\n        writer = csv.writer(csvfile)\\n        writer.writerows(predictions)\\n    print(\"Predictions saved to CSV file successfully!\")\\n\\n# Usage example\\n#save_predictions_to_csv(predictions_option1, \\'predictions_option1.csv\\')\\n\\nground_truth_labels = [job[0] for job in formatted_test]\\ndef save_ground_truth_to_csv(ground_truth, file_path):\\n    with open(file_path, \\'w\\', newline=\\'\\') as csvfile:\\n        writer = csv.writer(csvfile)\\n        for label in ground_truth:\\n            writer.writerow([label])\\n    print(\"Ground truth saved to CSV file successfully!\")\\n\\n# Usage example\\nsave_ground_truth_to_csv(ground_truth_labels, \\'ground_truth.csv\\')\\n'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import csv\n",
    "\n",
    "def save_predictions_to_csv(predictions, file_path):\n",
    "    with open(file_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(predictions)\n",
    "    print(\"Predictions saved to CSV file successfully!\")\n",
    "\n",
    "# Usage example\n",
    "#save_predictions_to_csv(predictions_option1, 'predictions_option1.csv')\n",
    "\n",
    "ground_truth_labels = [job[0] for job in formatted_test]\n",
    "def save_ground_truth_to_csv(ground_truth, file_path):\n",
    "    with open(file_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for label in ground_truth:\n",
    "            writer.writerow([label])\n",
    "    print(\"Ground truth saved to CSV file successfully!\")\n",
    "\n",
    "# Usage example\n",
    "save_ground_truth_to_csv(ground_truth_labels, 'ground_truth.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb2f9d8-f5d4-4fde-9acc-7c325db24e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "• True positive (TP): A test result that correctly indicates the presence of a condition or characteristic.\n",
    "• True negative (TN): A test result that correctly indicates the absence of a condition or characteristic.\n",
    "• False positive (FP): A test result that wrongly indicates that a particular condition or attribute is present.\n",
    "• False negative (FN): A test result that wrongly indicates that a particular condition or attribute is absent.\n",
    "\n",
    "'''\n",
    "\n",
    "# job[0] is the label/correct job category\n",
    "ground_truth_labels = [job[0] for job in formatted_test]\n",
    "\n",
    "# Option 1\n",
    "tp_option1 = 0\n",
    "fp_option1 = 0\n",
    "fn_option1 = 0\n",
    "tn_option1 = 0\n",
    "\n",
    "# Iterate over predicted job titles and ground truth labels\n",
    "for predicted_jobs, ground_truth in zip(predictions_option1, ground_truth_labels):\n",
    "    if ground_truth in predicted_jobs:\n",
    "        if ground_truth == predicted_jobs[0]:\n",
    "            tp_option1 += 1\n",
    "        else:\n",
    "            fp_option1 += 1\n",
    "    else:\n",
    "        if ground_truth == predicted_jobs[0]:\n",
    "            fn_option1 += 1\n",
    "        else:\n",
    "            tn_option1 += 1\n",
    "\n",
    "# Option 2\n",
    "tp_option2 = 0\n",
    "fp_option2 = 0\n",
    "fn_option2 = 0\n",
    "tn_option2 = 0\n",
    "\n",
    "# Iterate over predicted job titles and ground truth labels\n",
    "for predicted_jobs, ground_truth in zip(predictions_option2, ground_truth_labels):\n",
    "    if ground_truth in predicted_jobs:\n",
    "        if ground_truth == predicted_jobs[0]:\n",
    "            tp_option2 += 1\n",
    "        else:\n",
    "            fp_option2 += 1\n",
    "    else:\n",
    "        if ground_truth == predicted_jobs[0]:\n",
    "            fn_option2 += 1\n",
    "        else:\n",
    "            tn_option2 += 1\n",
    "\n",
    "# Option 3\n",
    "tp_option3 = 0\n",
    "fp_option3 = 0\n",
    "fn_option3 = 0\n",
    "tn_option3 = 0\n",
    "\n",
    "# Iterate over predicted job titles and ground truth labels\n",
    "for predicted_jobs, ground_truth in zip(predictions_option3, ground_truth_labels):\n",
    "    if ground_truth in predicted_jobs:\n",
    "        if ground_truth == predicted_jobs[0]:\n",
    "            tp_option3 += 1\n",
    "        else:\n",
    "            fp_option3 += 1\n",
    "    else:\n",
    "        if ground_truth == predicted_jobs[0]:\n",
    "            fn_option3 += 1\n",
    "        else:\n",
    "            tn_option3 += 1\n",
    "\n",
    "print(\"Option 1:\")\n",
    "print(\"True Positives (TP):\", tp_option1)\n",
    "print(\"False Positives (FP):\", fp_option1)\n",
    "print(\"False Negatives (FN):\", fn_option1)\n",
    "print(\"True Negatives (TN):\", tn_option1)\n",
    "\n",
    "print(\"\\nOption 2:\")\n",
    "print(\"True Positives (TP):\", tp_option2)\n",
    "print(\"False Positives (FP):\", fp_option2)\n",
    "print(\"False Negatives (FN):\", fn_option2)\n",
    "print(\"True Negatives (TN):\", tn_option2)\n",
    "\n",
    "print(\"\\nOption 3:\")\n",
    "print(\"True Positives (TP):\", tp_option3)\n",
    "print(\"False Positives (FP):\", fp_option3)\n",
    "print(\"False Negatives (FN):\", fn_option3)\n",
    "print(\"True Negatives (TN):\", tn_option3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "328836ca-2048-4a5a-bc28-bd5a02e16573",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "328836ca-2048-4a5a-bc28-bd5a02e16573",
    "outputId": "6dee34cc-b8b9-4c5a-99be-f450c3b514f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 1:\n",
      "True Positives (TP): 3\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 2763\n",
      "True Negatives (TN): 0\n",
      "\n",
      "Option 2:\n",
      "True Positives (TP): 3\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 2763\n",
      "True Negatives (TN): 0\n",
      "\n",
      "Option 3:\n",
      "True Positives (TP): 3\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 2763\n",
      "True Negatives (TN): 0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "• True positive (TP): A test result that correctly indicates the presence of a condition or characteristic.\n",
    "• True negative (TN): A test result that correctly indicates the absence of a condition or characteristic.\n",
    "• False positive (FP): A test result that wrongly indicates that a particular condition or attribute is present.\n",
    "• False negative (FN): A test result that wrongly indicates that a particular condition or attribute is absent.\n",
    "\n",
    "'''\n",
    "# job[0] is the label/correct job category\n",
    "ground_truth_labels = [job[0] for job in formatted_test]\n",
    "\n",
    "\n",
    "# Calculate the total number of samples in your dataset\n",
    "total_samples = len(df_test)\n",
    "\n",
    "# Option 1\n",
    "tp_option1 = len(set(predicted_jobs_option1) & set(ground_truth_labels))\n",
    "fp_option1 = len(predicted_jobs_option1) - tp_option1\n",
    "fn_option1 = len(ground_truth_labels) - tp_option1\n",
    "tn_option1 = total_samples - (tp_option1 + fp_option1 + fn_option1)\n",
    "\n",
    "print(\"Option 1:\")\n",
    "print(\"True Positives (TP):\", tp_option1)\n",
    "print(\"False Positives (FP):\", fp_option1)\n",
    "print(\"False Negatives (FN):\", fn_option1)\n",
    "print(\"True Negatives (TN):\", tn_option1)\n",
    "\n",
    "# Option 2\n",
    "tp_option2 = len(set(topJobs_option2) & set(ground_truth_labels))\n",
    "fp_option2 = len(topJobs_option2) - tp_option2\n",
    "fn_option2 = len(ground_truth_labels) - tp_option2\n",
    "tn_option2 = total_samples - (tp_option2 + fp_option2 + fn_option2)\n",
    "\n",
    "print(\"\\nOption 2:\")\n",
    "print(\"True Positives (TP):\", tp_option2)\n",
    "print(\"False Positives (FP):\", fp_option2)\n",
    "print(\"False Negatives (FN):\", fn_option2)\n",
    "print(\"True Negatives (TN):\", tn_option2)\n",
    "\n",
    "# Option 3\n",
    "tp_option3 = len(set(topJobs_option3) & set(ground_truth_labels))\n",
    "fp_option3 = len(topJobs_option3) - tp_option3\n",
    "fn_option3 = len(ground_truth_labels) - tp_option3\n",
    "tn_option3 = total_samples - (tp_option3 + fp_option3 + fn_option3)\n",
    "\n",
    "print(\"\\nOption 3:\")\n",
    "print(\"True Positives (TP):\", tp_option3)\n",
    "print(\"False Positives (FP):\", fp_option3)\n",
    "print(\"False Negatives (FN):\", fn_option3)\n",
    "print(\"True Negatives (TN):\", tn_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62110e-d19b-435c-ac3d-304ccf24742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "• Sensitivity, recall, hit rate, or true positive rate (TPR): The proportion of true positives out of all actual positive cases.\n",
    "'''\n",
    "\n",
    "# Calculate Sensitivity (TPR) for Option 1\n",
    "sensitivity_option1 = tp_option1 / (tp_option1 + fn_option1)\n",
    "\n",
    "# Calculate Sensitivity (TPR) for Option 2\n",
    "sensitivity_option2 = tp_option2 / (tp_option2 + fn_option2)\n",
    "\n",
    "# Calculate Sensitivity (TPR) for Option 3\n",
    "sensitivity_option3 = tp_option3 / (tp_option3 + fn_option3)\n",
    "\n",
    "print(\"Sensitivity (TPR) - Option 1:\", sensitivity_option1)\n",
    "print(\"Sensitivity (TPR) - Option 2:\", sensitivity_option2)\n",
    "print(\"Sensitivity (TPR) - Option 3:\", sensitivity_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2e5ad6-ba1e-44b5-bf1d-54d768a40fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "• Specificity, selectivity, or true negative rate (TNR): The proportion of true negatives out of all actual negative cases.\n",
    "'''\n",
    "# Calculate Specificity (TNR) for Option 1\n",
    "specificity_option1 = tn_option1 / (tn_option1 + fp_option1)\n",
    "\n",
    "# Calculate Specificity (TNR) for Option 2\n",
    "specificity_option2 = tn_option2 / (tn_option2 + fp_option2)\n",
    "\n",
    "# Calculate Specificity (TNR) for Option 3\n",
    "specificity_option3 = tn_option3 / (tn_option3 + fp_option3)\n",
    "\n",
    "print(\"Specificity (TNR) - Option 1:\", specificity_option1)\n",
    "print(\"Specificity (TNR) - Option 2:\", specificity_option2)\n",
    "print(\"Specificity (TNR) - Option 3:\", specificity_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f6e9a-bb78-436b-89f8-366851c4aaf0",
   "metadata": {
    "id": "b16f6e9a-bb78-436b-89f8-366851c4aaf0",
    "outputId": "9658cf1e-f519-4bff-9483-81e3e9fe6f36"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "• Precision or positive predictive value (PPV): The proportion of true positives out of all positive predictions made by the model.\n",
    "'''\n",
    "\n",
    "# Calculate Precision (PPV) for Option 1\n",
    "precision_option1 = tp_option1 / (tp_option1 + fp_option1)\n",
    "\n",
    "# Calculate Precision (PPV) for Option 2\n",
    "precision_option2 = tp_option2 / (tp_option2 + fp_option2)\n",
    "\n",
    "# Calculate Precision (PPV) for Option 3\n",
    "precision_option3 = tp_option3 / (tp_option3 + fp_option3)\n",
    "\n",
    "print(\"Precision (PPV) - Option 1:\", precision_option1)\n",
    "print(\"Precision (PPV) - Option 2:\", precision_option2)\n",
    "print(\"Precision (PPV) - Option 3:\", precision_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef591a1-0d98-4da9-9c8a-4e7c6e68dcf3",
   "metadata": {
    "id": "5ef591a1-0d98-4da9-9c8a-4e7c6e68dcf3"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "• Negative predictive value (NPV): The proportion of true negatives out of all negative predictions made by the model.\n",
    "'''\n",
    "\n",
    "# Calculate Negative Predictive Value (NPV) for Option 1\n",
    "npv_option1 = tn_option1 / (tn_option1 + fn_option1)\n",
    "\n",
    "# Calculate Negative Predictive Value (NPV) for Option 2\n",
    "npv_option2 = tn_option2 / (tn_option2 + fn_option2)\n",
    "\n",
    "# Calculate Negative Predictive Value (NPV) for Option 3\n",
    "npv_option3 = tn_option3 / (tn_option3 + fn_option3)\n",
    "\n",
    "print(\"Negative Predictive Value (NPV) - Option 1:\", npv_option1)\n",
    "print(\"Negative Predictive Value (NPV) - Option 2:\", npv_option2)\n",
    "print(\"Negative Predictive Value (NPV) - Option 3:\", npv_option3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af114b38-fc42-4537-b9dd-b036fb856e52",
   "metadata": {
    "id": "af114b38-fc42-4537-b9dd-b036fb856e52",
    "outputId": "68867287-7d87-47ef-bbbc-4d6c9927034a"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "• Miss rate or false negative rate (FNR): The proportion of false negatives out of all actual positive cases.\n",
    "'''\n",
    "\n",
    "# Calculate False Negative Rate (FNR) for Option 1\n",
    "fnr_option1 = fn_option1 / (fn_option1 + tp_option1)\n",
    "\n",
    "# Calculate False Negative Rate (FNR) for Option 2\n",
    "fnr_option2 = fn_option2 / (fn_option2 + tp_option2)\n",
    "\n",
    "# Calculate False Negative Rate (FNR) for Option 3\n",
    "fnr_option3 = fn_option3 / (fn_option3 + tp_option3)\n",
    "\n",
    "print(\"False Negative Rate (FNR) - Option 1:\", fnr_option1)\n",
    "print(\"False Negative Rate (FNR) - Option 2:\", fnr_option2)\n",
    "print(\"False Negative Rate (FNR) - Option 3:\", fnr_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22299028-f4a9-4028-9137-0b3602102396",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "• Fall-out or false positive rate (FPR): The proportion of false positives out of all actual negative cases.\n",
    "'''\n",
    "# Calculate False Positive Rate (FPR) for Option 1\n",
    "fpr_option1 = fp_option1 / (fp_option1 + tn_option1)\n",
    "\n",
    "# Calculate False Positive Rate (FPR) for Option 2\n",
    "fpr_option2 = fp_option2 / (fp_option2 + tn_option2)\n",
    "\n",
    "# Calculate False Positive Rate (FPR) for Option 3\n",
    "fpr_option3 = fp_option3 / (fp_option3 + tn_option3)\n",
    "\n",
    "print(\"False Positive Rate (FPR) - Option 1:\", fpr_option1)\n",
    "print(\"False Positive Rate (FPR) - Option 2:\", fpr_option2)\n",
    "print(\"False Positive Rate (FPR) - Option 3:\", fpr_option3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26fe6d8-340a-4377-abf1-6c20ffd53854",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "• False discovery rate (FDR): The proportion of false positives out of all positive predictions made by the model.\n",
    "'''\n",
    "# Calculate False Discovery Rate (FDR) for Option 1\n",
    "fdr_option1 = fp_option1 / (fp_option1 + tp_option1)\n",
    "\n",
    "# Calculate False Discovery Rate (FDR) for Option 2\n",
    "fdr_option2 = fp_option2 / (fp_option2 + tp_option2)\n",
    "\n",
    "# Calculate False Discovery Rate (FDR) for Option 3\n",
    "fdr_option3 = fp_option3 / (fp_option3 + tp_option3)\n",
    "\n",
    "print(\"False Discovery Rate (FDR) - Option 1:\", fdr_option1)\n",
    "print(\"False Discovery Rate (FDR) - Option 2:\", fdr_option2)\n",
    "print(\"False Discovery Rate (FDR) - Option 3:\", fdr_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d54d52-43ea-4a93-b19c-9cd80519456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "• False omission rate (FOR): The proportion of false negatives out of all negative predictions made by the model.\n",
    "'''\n",
    "# Calculate False Omission Rate (FOR) for Option 1\n",
    "for_option1 = fn_option1 / (fn_option1 + tn_option1)\n",
    "\n",
    "# Calculate False Omission Rate (FOR) for Option 2\n",
    "for_option2 = fn_option2 / (fn_option2 + tn_option2)\n",
    "\n",
    "# Calculate False Omission Rate (FOR) for Option 3\n",
    "for_option3 = fn_option3 / (fn_option3 + tn_option3)\n",
    "\n",
    "print(\"False Omission Rate (FOR) - Option 1:\", for_option1)\n",
    "print(\"False Omission Rate (FOR) - Option 2:\", for_option2)\n",
    "print(\"False Omission Rate (FOR) - Option 3:\", for_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d768ac07-66bf-4079-9fcb-592609ce98cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "• Positive likelihood ratio (LR+): The ratio of true positive rate to false positive rate.\n",
    "'''\n",
    "\n",
    "# Calculate True Positive Rate (TPR) for Option 1\n",
    "tpr_option1 = tp_option1 / (tp_option1 + fn_option1)\n",
    "\n",
    "# Calculate False Positive Rate (FPR) for Option 1\n",
    "fpr_option1 = fp_option1 / (fp_option1 + tn_option1)\n",
    "\n",
    "# Calculate True Positive Rate (TPR) for Option 2\n",
    "tpr_option2 = tp_option2 / (tp_option2 + fn_option2)\n",
    "\n",
    "# Calculate False Positive Rate (FPR) for Option 2\n",
    "fpr_option2 = fp_option2 / (fp_option2 + tn_option2)\n",
    "\n",
    "# Calculate True Positive Rate (TPR) for Option 3\n",
    "tpr_option3 = tp_option3 / (tp_option3 + fn_option3)\n",
    "\n",
    "# Calculate False Positive Rate (FPR) for Option 3\n",
    "fpr_option3 = fp_option3 / (fp_option3 + tn_option3)\n",
    "\n",
    "# Calculate Positive Likelihood Ratio (LR+) for Option 1\n",
    "lr_plus_option1 = tpr_option1 / fpr_option1\n",
    "\n",
    "# Calculate Positive Likelihood Ratio (LR+) for Option 2\n",
    "lr_plus_option2 = tpr_option2 / fpr_option2\n",
    "\n",
    "# Calculate Positive Likelihood Ratio (LR+) for Option 3\n",
    "lr_plus_option3 = tpr_option3 / fpr_option3\n",
    "\n",
    "print(\"Positive Likelihood Ratio (LR+) - Option 1:\", lr_plus_option1)\n",
    "print(\"Positive Likelihood Ratio (LR+) - Option 2:\", lr_plus_option2)\n",
    "print(\"Positive Likelihood Ratio (LR+) - Option 3:\", lr_plus_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589984a-4e4f-4995-afdb-6234b5408ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "• Negative likelihood ratio (LR-): The ratio of false negative rate to true negative rate.\n",
    "'''\n",
    "# Calculate False Negative Rate (FNR) for Option 1\n",
    "fnr_option1 = fn_option1 / (fn_option1 + tp_option1)\n",
    "\n",
    "# Calculate True Negative Rate (TNR) for Option 1\n",
    "tnr_option1 = tn_option1 / (tn_option1 + fp_option1)\n",
    "\n",
    "# Calculate False Negative Rate (FNR) for Option 2\n",
    "fnr_option2 = fn_option2 / (fn_option2 + tp_option2)\n",
    "\n",
    "# Calculate True Negative Rate (TNR) for Option 2\n",
    "tnr_option2 = tn_option2 / (tn_option2 + fp_option2)\n",
    "\n",
    "# Calculate False Negative Rate (FNR) for Option 3\n",
    "fnr_option3 = fn_option3 / (fn_option3 + tp_option3)\n",
    "\n",
    "# Calculate True Negative Rate (TNR) for Option 3\n",
    "tnr_option3 = tn_option3 / (tn_option3 + fp_option3)\n",
    "\n",
    "# Calculate Negative Likelihood Ratio (LR-) for Option 1\n",
    "lr_minus_option1 = fnr_option1 / tnr_option1\n",
    "\n",
    "# Calculate Negative Likelihood Ratio (LR-) for Option 2\n",
    "lr_minus_option2 = fnr_option2 / tnr_option2\n",
    "\n",
    "# Calculate Negative Likelihood Ratio (LR-) for Option 3\n",
    "lr_minus_option3 = fnr_option3 / tnr_option3\n",
    "\n",
    "print(\"Negative Likelihood Ratio (LR-) - Option 1:\", lr_minus_option1)\n",
    "print(\"Negative Likelihood Ratio (LR-) - Option 2:\", lr_minus_option2)\n",
    "print(\"Negative Likelihood Ratio (LR-) - Option 3:\", lr_minus_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0193a6fd-616d-429b-895d-d5904a664438",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "• Prevalence threshold (PT): A threshold value that balances sensitivity and specificity.\n",
    "'''\n",
    "# Calculate TPR and TNR for Option 1\n",
    "tpr_option1 = tp_option1 / (tp_option1 + fn_option1)\n",
    "tnr_option1 = tn_option1 / (tn_option1 + fp_option1)\n",
    "\n",
    "# Calculate Prevalence Threshold (PT) for Option 1\n",
    "pt_option1 = (math.sqrt(tpr_option1 * (1 - tnr_option1)) + tnr_option1 - 1) / (tpr_option1 + tnr_option1 - 1)\n",
    "\n",
    "print(\"Prevalence Threshold (PT) - Option 1:\", pt_option1)\n",
    "\n",
    "# Calculate TPR and TNR for Option 2\n",
    "tpr_option2 = tp_option2 / (tp_option2 + fn_option2)\n",
    "tnr_option2 = tn_option2 / (tn_option2 + fp_option2)\n",
    "\n",
    "# Calculate Prevalence Threshold (PT) for Option 2\n",
    "pt_option2 = (math.sqrt(tpr_option2 * (1 - tnr_option2)) + tnr_option2 - 1) / (tpr_option2 + tnr_option2 - 1)\n",
    "\n",
    "print(\"Prevalence Threshold (PT) - Option 2:\", pt_option2)\n",
    "\n",
    "# Calculate TPR and TNR for Option 3\n",
    "tpr_option3 = tp_option3 / (tp_option3 + fn_option3)\n",
    "tnr_option3 = tn_option3 / (tn_option3 + fp_option3)\n",
    "\n",
    "# Calculate Prevalence Threshold (PT) for Option 3\n",
    "pt_option3 = (math.sqrt(tpr_option3 * (1 - tnr_option3)) + tnr_option3 - 1) / (tpr_option3 + tnr_option3 - 1)\n",
    "\n",
    "print(\"Prevalence Threshold (PT) - Option 3:\", pt_option3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6c02df-0795-4c31-bfb7-7f0fd98c967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "• Threat score (TS) or critical success index (CSI): The proportion of true positives out of all positive and negative predictions made by the model.\n",
    "'''\n",
    "\n",
    "# Calculate Threat Score (TS) / Critical Success Index (CSI) for Option 1\n",
    "ts_option1 = tp_option1 / (tp_option1 + fp_option1 + fn_option1)\n",
    "\n",
    "print(\"Threat Score (TS) - Option 1:\", ts_option1)\n",
    "\n",
    "# Calculate Threat Score (TS) / Critical Success Index (CSI) for Option 2\n",
    "ts_option2 = tp_option2 / (tp_option2 + fp_option2 + fn_option2)\n",
    "\n",
    "print(\"Threat Score (TS) - Option 2:\", ts_option2)\n",
    "\n",
    "# Calculate Threat Score (TS) / Critical Success Index (CSI) for Option 3\n",
    "ts_option3 = tp_option3 / (tp_option3 + fp_option3 + fn_option3)\n",
    "\n",
    "print(\"Threat Score (TS) - Option 3:\", ts_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bb4611-258f-4677-8a7f-875f55ddef4b",
   "metadata": {
    "id": "73bb4611-258f-4677-8a7f-875f55ddef4b"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "• Prevalence: The proportion of positive cases in the data.\n",
    "'''\n",
    "# Calculate Prevalence for Option 1\n",
    "prevalence_option1 = tp_option1 / total_samples\n",
    "\n",
    "print(\"Prevalence - Option 1:\", prevalence_option1)\n",
    "\n",
    "# Calculate Prevalence for Option 2\n",
    "prevalence_option2 = tp_option2 / total_samples\n",
    "\n",
    "print(\"Prevalence - Option 2:\", prevalence_option2)\n",
    "\n",
    "# Calculate Prevalence for Option 3\n",
    "prevalence_option3 = tp_option3 / total_samples\n",
    "\n",
    "print(\"Prevalence - Option 3:\", prevalence_option3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01bb76c-037e-476b-be33-8591222bd8b9",
   "metadata": {
    "id": "f01bb76c-037e-476b-be33-8591222bd8b9",
    "outputId": "4671c668-0d2b-4887-a761-aacd6688a4d7"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "• Accuracy (ACC): The proportion of correct predictions (both true positives and true negatives) out of all predictions made by the model.\n",
    "'''\n",
    "\n",
    "# Calculate Accuracy for Option 1\n",
    "accuracy_option1 = (tp_option1 + tn_option1) / total_samples\n",
    "\n",
    "print(\"Accuracy - Option 1:\", accuracy_option1)\n",
    "\n",
    "# Calculate Accuracy for Option 2\n",
    "accuracy_option2 = (tp_option2 + tn_option2) / total_samples\n",
    "\n",
    "print(\"Accuracy - Option 2:\", accuracy_option2)\n",
    "\n",
    "# Calculate Accuracy for Option 3\n",
    "accuracy_option3 = (tp_option3 + tn_option3) / total_samples\n",
    "\n",
    "print(\"Accuracy - Option 3:\", accuracy_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6cdf5d-ed5b-43b2-945b-383f4421b2b6",
   "metadata": {
    "id": "be6cdf5d-ed5b-43b2-945b-383f4421b2b6"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "• Balanced accuracy (BA): The average of sensitivity and specificity.\n",
    "'''\n",
    "# Calculate Specificity (TNR) for each option\n",
    "tnr_option1 = tn_option1 / (tn_option1 + fp_option1)\n",
    "tnr_option2 = tn_option2 / (tn_option2 + fp_option2)\n",
    "tnr_option3 = tn_option3 / (tn_option3 + fp_option3)\n",
    "\n",
    "# Calculate Balanced Accuracy for Option 1\n",
    "ba_option1 = (tpr_option1 + tnr_option1) / 2\n",
    "\n",
    "print(\"Balanced Accuracy - Option 1:\", ba_option1)\n",
    "\n",
    "# Calculate Balanced Accuracy for Option 2\n",
    "ba_option2 = (tpr_option2 + tnr_option2) / 2\n",
    "\n",
    "print(\"Balanced Accuracy - Option 2:\", ba_option2)\n",
    "\n",
    "# Calculate Balanced Accuracy for Option 3\n",
    "ba_option3 = (tpr_option3 + tnr_option3) / 2\n",
    "\n",
    "print(\"Balanced Accuracy - Option 3:\", ba_option3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175ad77b-4f27-4131-aa9f-0522b1cf6280",
   "metadata": {
    "id": "175ad77b-4f27-4131-aa9f-0522b1cf6280"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "• F1 score: The harmonic mean of precision and sensitivity, providing a single metric to balance both.\n",
    "'''\n",
    "\n",
    "# Calculate F1 Score for Option 1\n",
    "precision_option1 = tp_option1 / (tp_option1 + fp_option1)\n",
    "recall_option1 = tp_option1 / (tp_option1 + fn_option1)\n",
    "f1_score_option1 = 2 * (precision_option1 * recall_option1) / (precision_option1 + recall_option1)\n",
    "\n",
    "print(\"F1 Score - Option 1:\", f1_score_option1)\n",
    "\n",
    "# Calculate F1 Score for Option 2\n",
    "precision_option2 = tp_option2 / (tp_option2 + fp_option2)\n",
    "recall_option2 = tp_option2 / (tp_option2 + fn_option2)\n",
    "f1_score_option2 = 2 * (precision_option2 * recall_option2) / (precision_option2 + recall_option2)\n",
    "\n",
    "print(\"F1 Score - Option 2:\", f1_score_option2)\n",
    "\n",
    "# Calculate F1 Score for Option 3\n",
    "precision_option3 = tp_option3 / (tp_option3 + fp_option3)\n",
    "recall_option3 = tp_option3 / (tp_option3 + fn_option3)\n",
    "f1_score_option3 = 2 * (precision_option3 * recall_option3) / (precision_option3 + recall_option3)\n",
    "\n",
    "print(\"F1 Score - Option 3:\", f1_score_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c0457-9e97-485c-b125-1d0a80e6b7e6",
   "metadata": {
    "id": "695c0457-9e97-485c-b125-1d0a80e6b7e6",
    "outputId": "8c1576ae-e06e-4d10-befc-fd95b2ffdc77"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "• Phi coefficient (φ) or Matthews correlation coefficient (MCC): A correlation coefficient that takes into account true positives, true negatives, false positives, and false negatives.\n",
    "'''\n",
    "\n",
    "# Calculate Phi coefficient (MCC) for Option 1\n",
    "mcc_option1 = (tp_option1 * tn_option1 - fp_option1 * fn_option1) / ((tp_option1 + fp_option1) * (tp_option1 + fn_option1) * (tn_option1 + fp_option1) * (tn_option1 + fn_option1)) ** 0.5\n",
    "\n",
    "print(\"Phi Coefficient (MCC) - Option 1:\", mcc_option1)\n",
    "\n",
    "# Calculate Phi coefficient (MCC) for Option 2\n",
    "mcc_option2 = (tp_option2 * tn_option2 - fp_option2 * fn_option2) / ((tp_option2 + fp_option2) * (tp_option2 + fn_option2) * (tn_option2 + fp_option2) * (tn_option2 + fn_option2)) ** 0.5\n",
    "\n",
    "print(\"Phi Coefficient (MCC) - Option 2:\", mcc_option2)\n",
    "\n",
    "# Calculate Phi coefficient (MCC) for Option 3\n",
    "mcc_option3 = (tp_option3 * tn_option3 - fp_option3 * fn_option3) / ((tp_option3 + fp_option3) * (tp_option3 + fn_option3) * (tn_option3 + fp_option3) * (tn_option3 + fn_option3)) ** 0.5\n",
    "\n",
    "print(\"Phi Coefficient (MCC) - Option 3:\", mcc_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db0488-11a9-49fe-9b7c-94227a7727e7",
   "metadata": {
    "id": "e5db0488-11a9-49fe-9b7c-94227a7727e7"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "• Fowlkes-Mallows index (FM): A geometric mean of precision and sensitivity.\n",
    "'''\n",
    "# Calculate Fowlkes-Mallows index (FM) for Option 1\n",
    "fm_option1 = (tp_option1 * tp_option1 / ((tp_option1 + fp_option1) * (tp_option1 + fn_option1))) ** 0.5\n",
    "\n",
    "print(\"Fowlkes-Mallows index (FM) - Option 1:\", fm_option1)\n",
    "\n",
    "# Calculate Fowlkes-Mallows index (FM) for Option 2\n",
    "fm_option2 = (tp_option2 * tp_option2 / ((tp_option2 + fp_option2) * (tp_option2 + fn_option2))) ** 0.5\n",
    "\n",
    "print(\"Fowlkes-Mallows index (FM) - Option 2:\", fm_option2)\n",
    "\n",
    "# Calculate Fowlkes-Mallows index (FM) for Option 3\n",
    "fm_option3 = (tp_option3 * tp_option3 / ((tp_option3 + fp_option3) * (tp_option3 + fn_option3))) ** 0.5\n",
    "\n",
    "print(\"Fowlkes-Mallows index (FM) - Option 3:\", fm_option3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ab1308-e5c4-4f69-b114-9d0d107b2e19",
   "metadata": {
    "id": "65ab1308-e5c4-4f69-b114-9d0d107b2e19"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b31c35-8bae-40a3-87a5-6c94732b05e7",
   "metadata": {
    "id": "13b31c35-8bae-40a3-87a5-6c94732b05e7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a6ead-8519-4bd5-89f0-d05b0ec21d4b",
   "metadata": {
    "id": "d11a6ead-8519-4bd5-89f0-d05b0ec21d4b"
   },
   "outputs": [],
   "source": [
    "•\tInformedness or bookmaker informedness (BM): The sum of true positive rate and true negative rate, minus 1.\n",
    "•\tMarkedness (MK) or deltaP (Δp): The sum of positive predictive value and negative predictive value, minus 1."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
